[CostModel] Added NVLink: 1024
(NVLink) Added cost: 256000 (BW: 125, count: 1024, unit_cost: 100)
[CostModel] Added NVLink: 128
(NVLink) Added cost: 6400 (BW: 25, count: 128, unit_cost: 100)
[CostModel] Added NVSwitch: 4
(Resource) Added cost: 94792 (count: 4, unit_cost: 23698)
[CostModel] Added NVLink: 512
(NVLink) Added cost: 25600 (BW: 25, count: 512, unit_cost: 100)
[CostModel] Added NVSwitch: 13
(Resource) Added cost: 308074 (count: 13, unit_cost: 23698)
[CostModel] Added NVLink: 1024
(NVLink) Added cost: 102400 (BW: 50, count: 1024, unit_cost: 100)
[CostModel] Added NVSwitch: 26
(Resource) Added cost: 616148 (count: 26, unit_cost: 23698)
[CostModel] Added NPU: 1024
(Resource) Added cost: 0 (count: 1024, unit_cost: 0)
Success in opening system file
Var is: scheduling-policy: ,val is: LIFO
Var is: endpoint-delay: ,val is: 1
Var is: active-chunks-per-dimension: ,val is: 1
Var is: preferred-dataset-splits: ,val is: 64
Var is: boost-mode: ,val is: 1
Var is: all-reduce-implementation: ,val is: ring_halvingDoubling_halvingDoubling_halvingDoubling
Var is: all-gather-implementation: ,val is: ring_halvingDoubling_halvingDoubling_halvingDoubling
Var is: reduce-scatter-implementation: ,val is: ring_halvingDoubling_halvingDoubling_halvingDoubling
Var is: all-to-all-implementation: ,val is: direct_direct_direct_direct
Var is: collective-optimization: ,val is: localBWAware
Var is: intra-dimension-scheduling: ,val is: smallestFirst
Var is: inter-dimension-scheduling: ,val is: offlineGreedy
Var is:  ,val is: offlineGreedy
The final active chunks per dimension after allocating to queues is: 100000000
ring of node 0, id: 0 dimension: local total nodes in ring: 4 index in ring: 0 offset: 1total nodes in ring: 4
ring of node 0, id: 0 dimension: local total nodes in ring: 4 index in ring: 0 offset: 4total nodes in ring: 4
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 16total nodes in ring: 8
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 128total nodes in ring: 8
ring of node 0, id: 0 dimension: local total nodes in ring: 4 index in ring: 0 offset: 1total nodes in ring: 4
ring of node 0, id: 0 dimension: local total nodes in ring: 4 index in ring: 0 offset: 4total nodes in ring: 4
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 16total nodes in ring: 8
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 128total nodes in ring: 8
ring of node 0, id: 0 dimension: local total nodes in ring: 4 index in ring: 0 offset: 1total nodes in ring: 4
ring of node 0, id: 0 dimension: local total nodes in ring: 4 index in ring: 0 offset: 4total nodes in ring: 4
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 16total nodes in ring: 8
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 128total nodes in ring: 8
ring of node 0, id: 0 dimension: local total nodes in ring: 4 index in ring: 0 offset: 1total nodes in ring: 4
ring of node 0, id: 0 dimension: local total nodes in ring: 4 index in ring: 0 offset: 4total nodes in ring: 4
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 16total nodes in ring: 8
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 128total nodes in ring: 8
total nodes: 1024
LogGP model, the local reduction delay is: 1
LogGP model, the local reduction delay is: 1
LogGP model, the local reduction delay is: 1
LogGP model, the local reduction delay is: 1
Shared bus modeling enabled? false
LogGP model, the L is:0 ,o is: 0 ,g is: 0 ,G is: 0.0038
communication delay (in the case of disabled shared bus): 1
Shared bus modeling enabled? false
LogGP model, the L is:0 ,o is: 0 ,g is: 0 ,G is: 0
communication delay (in the case of disabled shared bus): 1
Success in opening workload file
id: conv1 , depen: -1 , wg_comp_time: 32291
id: layer_64_1_conv4 , depen: -1 , wg_comp_time: 7488
id: layer_64_1_conv1 , depen: -1 , wg_comp_time: 7488
id: layer_64_1_conv2 , depen: -1 , wg_comp_time: 14144
id: layer_64_1_conv3 , depen: -1 , wg_comp_time: 7488
id: layer_64_2_conv1 , depen: -1 , wg_comp_time: 9984
id: layer_64_2_conv2 , depen: -1 , wg_comp_time: 14144
id: layer_64_2_conv3 , depen: -1 , wg_comp_time: 7488
id: layer_64_3_conv1 , depen: -1 , wg_comp_time: 9984
id: layer_64_3_conv2 , depen: -1 , wg_comp_time: 14144
id: layer_64_3_conv3 , depen: -1 , wg_comp_time: 7488
id: layer_128_1_conv4 , depen: -1 , wg_comp_time: 6144
id: layer_128_1_conv1 , depen: -1 , wg_comp_time: 9984
id: layer_128_1_conv2 , depen: -1 , wg_comp_time: 6656
id: layer_128_1_conv3 , depen: -1 , wg_comp_time: 5120
id: layer_128_2_conv1 , depen: -1 , wg_comp_time: 4096
id: layer_128_2_conv2 , depen: -1 , wg_comp_time: 6656
id: layer_128_2_conv3 , depen: -1 , wg_comp_time: 5120
id: layer_128_3_conv1 , depen: -1 , wg_comp_time: 4096
id: layer_128_3_conv2 , depen: -1 , wg_comp_time: 6656
id: layer_128_3_conv3 , depen: -1 , wg_comp_time: 5120
id: layer_128_4_conv1 , depen: -1 , wg_comp_time: 4096
id: layer_128_4_conv2 , depen: -1 , wg_comp_time: 6656
id: layer_128_4_conv3 , depen: -1 , wg_comp_time: 5120
id: layer_256_1_conv4 , depen: -1 , wg_comp_time: 3856
id: layer_256_1_conv1 , depen: -1 , wg_comp_time: 4096
id: layer_256_1_conv2 , depen: -1 , wg_comp_time: 2756
id: layer_256_1_conv3 , depen: -1 , wg_comp_time: 2832
id: layer_256_2_conv1 , depen: -1 , wg_comp_time: 1476
id: layer_256_2_conv2 , depen: -1 , wg_comp_time: 2756
id: layer_256_2_conv3 , depen: -1 , wg_comp_time: 2832
id: layer_256_3_conv1 , depen: -1 , wg_comp_time: 1476
id: layer_256_3_conv2 , depen: -1 , wg_comp_time: 2756
id: layer_256_3_conv3 , depen: -1 , wg_comp_time: 2832
id: layer_256_4_conv1 , depen: -1 , wg_comp_time: 1476
id: layer_256_4_conv2 , depen: -1 , wg_comp_time: 2756
id: layer_256_4_conv3 , depen: -1 , wg_comp_time: 2832
id: layer_256_5_conv1 , depen: -1 , wg_comp_time: 1476
id: layer_256_5_conv2 , depen: -1 , wg_comp_time: 2756
id: layer_256_5_conv3 , depen: -1 , wg_comp_time: 2832
id: layer_256_6_conv1 , depen: -1 , wg_comp_time: 1476
id: layer_256_6_conv2 , depen: -1 , wg_comp_time: 2756
id: layer_256_6_conv3 , depen: -1 , wg_comp_time: 2832
id: layer_512_1_conv4 , depen: -1 , wg_comp_time: 10632
id: layer_512_1_conv1 , depen: -1 , wg_comp_time: 2952
id: layer_512_1_conv2 , depen: -1 , wg_comp_time: 9826
id: layer_512_1_conv3 , depen: -1 , wg_comp_time: 6536
id: layer_512_2_conv1 , depen: -1 , wg_comp_time: 4706
id: layer_512_2_conv2 , depen: -1 , wg_comp_time: 9826
id: layer_512_2_conv3 , depen: -1 , wg_comp_time: 6536
id: layer_512_3_conv1 , depen: -1 , wg_comp_time: 4706
id: layer_512_3_conv2 , depen: -1 , wg_comp_time: 9826
id: layer_512_3_conv3 , depen: -1 , wg_comp_time: 6536
id: fc1000 , depen: -1 , wg_comp_time: 9220
type: DATA ,num passes: 1 ,lines: 54 compute scale: 0.26 ,comm scale: 100
stat path: /Users/aakashsharma/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/work/astra-sim/examples/../results/DDL_themis/Resnet50_DataParallel/ ,total rows: 6 ,stat row: 4
Themis is configured with the following parameters: 
Dim size: 4, 4, 8, 8, 
BW per dim: 250, 200, 100, 50, 

Node 5 has been totally disabled
Node 6 has been totally disabled
Node 7 has been totally disabled
Node 9 has been totally disabled
Node 10 has been totally disabled
Node 11 has been totally disabled
Node 13 has been totally disabled
Node 14 has been totally disabled
Node 15 has been totally disabled
Node 17 has been totally disabled
Node 18 has been totally disabled
Node 19 has been totally disabled
Node 20 has been totally disabled
Node 21 has been totally disabled
Node 22 has been totally disabled
Node 23 has been totally disabled
Node 24 has been totally disabled
Node 25 has been totally disabled
Node 26 has been totally disabled
Node 27 has been totally disabled
Node 28 has been totally disabled
Node 29 has been totally disabled
Node 30 has been totally disabled
Node 31 has been totally disabled
Node 33 has been totally disabled
Node 34 has been totally disabled
Node 35 has been totally disabled
Node 36 has been totally disabled
Node 37 has been totally disabled
Node 38 has been totally disabled
Node 39 has been totally disabled
Node 40 has been totally disabled
Node 41 has been totally disabled
Node 42 has been totally disabled
Node 43 has been totally disabled
Node 44 has been totally disabled
Node 45 has been totally disabled
Node 46 has been totally disabled
Node 47 has been totally disabled
Node 49 has been totally disabled
Node 50 has been totally disabled
Node 51 has been totally disabled
Node 52 has been totally disabled
Node 53 has been totally disabled
Node 54 has been totally disabled
Node 55 has been totally disabled
Node 56 has been totally disabled
Node 57 has been totally disabled
Node 58 has been totally disabled
Node 59 has been totally disabled
Node 60 has been totally disabled
Node 61 has been totally disabled
Node 62 has been totally disabled
Node 63 has been totally disabled
Node 65 has been totally disabled
Node 66 has been totally disabled
Node 67 has been totally disabled
Node 68 has been totally disabled
Node 69 has been totally disabled
Node 70 has been totally disabled
Node 71 has been totally disabled
Node 72 has been totally disabled
Node 73 has been totally disabled
Node 74 has been totally disabled
Node 75 has been totally disabled
Node 76 has been totally disabled
Node 77 has been totally disabled
Node 78 has been totally disabled
Node 79 has been totally disabled
Node 81 has been totally disabled
Node 82 has been totally disabled
Node 83 has been totally disabled
Node 84 has been totally disabled
Node 85 has been totally disabled
Node 86 has been totally disabled
Node 87 has been totally disabled
Node 88 has been totally disabled
Node 89 has been totally disabled
Node 90 has been totally disabled
Node 91 has been totally disabled
Node 92 has been totally disabled
Node 93 has been totally disabled
Node 94 has been totally disabled
Node 95 has been totally disabled
Node 97 has been totally disabled
Node 98 has been totally disabled
Node 99 has been totally disabled
Node 100 has been totally disabled
Node 101 has been totally disabled
Node 102 has been totally disabled
Node 103 has been totally disabled
Node 104 has been totally disabled
Node 105 has been totally disabled
Node 106 has been totally disabled
Node 107 has been totally disabled
Node 108 has been totally disabled
Node 109 has been totally disabled
Node 110 has been totally disabled
Node 111 has been totally disabled
Node 113 has been totally disabled
Node 114 has been totally disabled
Node 115 has been totally disabled
Node 116 has been totally disabled
Node 117 has been totally disabled
Node 118 has been totally disabled
Node 119 has been totally disabled
Node 120 has been totally disabled
Node 121 has been totally disabled
Node 122 has been totally disabled
Node 123 has been totally disabled
Node 124 has been totally disabled
Node 125 has been totally disabled
Node 126 has been totally disabled
Node 127 has been totally disabled
Node 129 has been totally disabled
Node 130 has been totally disabled
Node 131 has been totally disabled
Node 132 has been totally disabled
Node 133 has been totally disabled
Node 134 has been totally disabled
Node 135 has been totally disabled
Node 136 has been totally disabled
Node 137 has been totally disabled
Node 138 has been totally disabled
Node 139 has been totally disabled
Node 140 has been totally disabled
Node 141 has been totally disabled
Node 142 has been totally disabled
Node 143 has been totally disabled
Node 144 has been totally disabled
Node 145 has been totally disabled
Node 146 has been totally disabled
Node 147 has been totally disabled
Node 148 has been totally disabled
Node 149 has been totally disabled
Node 150 has been totally disabled
Node 151 has been totally disabled
Node 152 has been totally disabled
Node 153 has been totally disabled
Node 154 has been totally disabled
Node 155 has been totally disabled
Node 156 has been totally disabled
Node 157 has been totally disabled
Node 158 has been totally disabled
Node 159 has been totally disabled
Node 160 has been totally disabled
Node 161 has been totally disabled
Node 162 has been totally disabled
Node 163 has been totally disabled
Node 164 has been totally disabled
Node 165 has been totally disabled
Node 166 has been totally disabled
Node 167 has been totally disabled
Node 168 has been totally disabled
Node 169 has been totally disabled
Node 170 has been totally disabled
Node 171 has been totally disabled
Node 172 has been totally disabled
Node 173 has been totally disabled
Node 174 has been totally disabled
Node 175 has been totally disabled
Node 176 has been totally disabled
Node 177 has been totally disabled
Node 178 has been totally disabled
Node 179 has been totally disabled
Node 180 has been totally disabled
Node 181 has been totally disabled
Node 182 has been totally disabled
Node 183 has been totally disabled
Node 184 has been totally disabled
Node 185 has been totally disabled
Node 186 has been totally disabled
Node 187 has been totally disabled
Node 188 has been totally disabled
Node 189 has been totally disabled
Node 190 has been totally disabled
Node 191 has been totally disabled
Node 192 has been totally disabled
Node 193 has been totally disabled
Node 194 has been totally disabled
Node 195 has been totally disabled
Node 196 has been totally disabled
Node 197 has been totally disabled
Node 198 has been totally disabled
Node 199 has been totally disabled
Node 200 has been totally disabled
Node 201 has been totally disabled
Node 202 has been totally disabled
Node 203 has been totally disabled
Node 204 has been totally disabled
Node 205 has been totally disabled
Node 206 has been totally disabled
Node 207 has been totally disabled
Node 208 has been totally disabled
Node 209 has been totally disabled
Node 210 has been totally disabled
Node 211 has been totally disabled
Node 212 has been totally disabled
Node 213 has been totally disabled
Node 214 has been totally disabled
Node 215 has been totally disabled
Node 216 has been totally disabled
Node 217 has been totally disabled
Node 218 has been totally disabled
Node 219 has been totally disabled
Node 220 has been totally disabled
Node 221 has been totally disabled
Node 222 has been totally disabled
Node 223 has been totally disabled
Node 224 has been totally disabled
Node 225 has been totally disabled
Node 226 has been totally disabled
Node 227 has been totally disabled
Node 228 has been totally disabled
Node 229 has been totally disabled
Node 230 has been totally disabled
Node 231 has been totally disabled
Node 232 has been totally disabled
Node 233 has been totally disabled
Node 234 has been totally disabled
Node 235 has been totally disabled
Node 236 has been totally disabled
Node 237 has been totally disabled
Node 238 has been totally disabled
Node 239 has been totally disabled
Node 240 has been totally disabled
Node 241 has been totally disabled
Node 242 has been totally disabled
Node 243 has been totally disabled
Node 244 has been totally disabled
Node 245 has been totally disabled
Node 246 has been totally disabled
Node 247 has been totally disabled
Node 248 has been totally disabled
Node 249 has been totally disabled
Node 250 has been totally disabled
Node 251 has been totally disabled
Node 252 has been totally disabled
Node 253 has been totally disabled
Node 254 has been totally disabled
Node 255 has been totally disabled
Node 257 has been totally disabled
Node 258 has been totally disabled
Node 259 has been totally disabled
Node 260 has been totally disabled
Node 261 has been totally disabled
Node 262 has been totally disabled
Node 263 has been totally disabled
Node 264 has been totally disabled
Node 265 has been totally disabled
Node 266 has been totally disabled
Node 267 has been totally disabled
Node 268 has been totally disabled
Node 269 has been totally disabled
Node 270 has been totally disabled
Node 271 has been totally disabled
Node 272 has been totally disabled
Node 273 has been totally disabled
Node 274 has been totally disabled
Node 275 has been totally disabled
Node 276 has been totally disabled
Node 277 has been totally disabled
Node 278 has been totally disabled
Node 279 has been totally disabled
Node 280 has been totally disabled
Node 281 has been totally disabled
Node 282 has been totally disabled
Node 283 has been totally disabled
Node 284 has been totally disabled
Node 285 has been totally disabled
Node 286 has been totally disabled
Node 287 has been totally disabled
Node 288 has been totally disabled
Node 289 has been totally disabled
Node 290 has been totally disabled
Node 291 has been totally disabled
Node 292 has been totally disabled
Node 293 has been totally disabled
Node 294 has been totally disabled
Node 295 has been totally disabled
Node 296 has been totally disabled
Node 297 has been totally disabled
Node 298 has been totally disabled
Node 299 has been totally disabled
Node 300 has been totally disabled
Node 301 has been totally disabled
Node 302 has been totally disabled
Node 303 has been totally disabled
Node 304 has been totally disabled
Node 305 has been totally disabled
Node 306 has been totally disabled
Node 307 has been totally disabled
Node 308 has been totally disabled
Node 309 has been totally disabled
Node 310 has been totally disabled
Node 311 has been totally disabled
Node 312 has been totally disabled
Node 313 has been totally disabled
Node 314 has been totally disabled
Node 315 has been totally disabled
Node 316 has been totally disabled
Node 317 has been totally disabled
Node 318 has been totally disabled
Node 319 has been totally disabled
Node 320 has been totally disabled
Node 321 has been totally disabled
Node 322 has been totally disabled
Node 323 has been totally disabled
Node 324 has been totally disabled
Node 325 has been totally disabled
Node 326 has been totally disabled
Node 327 has been totally disabled
Node 328 has been totally disabled
Node 329 has been totally disabled
Node 330 has been totally disabled
Node 331 has been totally disabled
Node 332 has been totally disabled
Node 333 has been totally disabled
Node 334 has been totally disabled
Node 335 has been totally disabled
Node 336 has been totally disabled
Node 337 has been totally disabled
Node 338 has been totally disabled
Node 339 has been totally disabled
Node 340 has been totally disabled
Node 341 has been totally disabled
Node 342 has been totally disabled
Node 343 has been totally disabled
Node 344 has been totally disabled
Node 345 has been totally disabled
Node 346 has been totally disabled
Node 347 has been totally disabled
Node 348 has been totally disabled
Node 349 has been totally disabled
Node 350 has been totally disabled
Node 351 has been totally disabled
Node 352 has been totally disabled
Node 353 has been totally disabled
Node 354 has been totally disabled
Node 355 has been totally disabled
Node 356 has been totally disabled
Node 357 has been totally disabled
Node 358 has been totally disabled
Node 359 has been totally disabled
Node 360 has been totally disabled
Node 361 has been totally disabled
Node 362 has been totally disabled
Node 363 has been totally disabled
Node 364 has been totally disabled
Node 365 has been totally disabled
Node 366 has been totally disabled
Node 367 has been totally disabled
Node 368 has been totally disabled
Node 369 has been totally disabled
Node 370 has been totally disabled
Node 371 has been totally disabled
Node 372 has been totally disabled
Node 373 has been totally disabled
Node 374 has been totally disabled
Node 375 has been totally disabled
Node 376 has been totally disabled
Node 377 has been totally disabled
Node 378 has been totally disabled
Node 379 has been totally disabled
Node 380 has been totally disabled
Node 381 has been totally disabled
Node 382 has been totally disabled
Node 383 has been totally disabled
Node 385 has been totally disabled
Node 386 has been totally disabled
Node 387 has been totally disabled
Node 388 has been totally disabled
Node 389 has been totally disabled
Node 390 has been totally disabled
Node 391 has been totally disabled
Node 392 has been totally disabled
Node 393 has been totally disabled
Node 394 has been totally disabled
Node 395 has been totally disabled
Node 396 has been totally disabled
Node 397 has been totally disabled
Node 398 has been totally disabled
Node 399 has been totally disabled
Node 400 has been totally disabled
Node 401 has been totally disabled
Node 402 has been totally disabled
Node 403 has been totally disabled
Node 404 has been totally disabled
Node 405 has been totally disabled
Node 406 has been totally disabled
Node 407 has been totally disabled
Node 408 has been totally disabled
Node 409 has been totally disabled
Node 410 has been totally disabled
Node 411 has been totally disabled
Node 412 has been totally disabled
Node 413 has been totally disabled
Node 414 has been totally disabled
Node 415 has been totally disabled
Node 416 has been totally disabled
Node 417 has been totally disabled
Node 418 has been totally disabled
Node 419 has been totally disabled
Node 420 has been totally disabled
Node 421 has been totally disabled
Node 422 has been totally disabled
Node 423 has been totally disabled
Node 424 has been totally disabled
Node 425 has been totally disabled
Node 426 has been totally disabled
Node 427 has been totally disabled
Node 428 has been totally disabled
Node 429 has been totally disabled
Node 430 has been totally disabled
Node 431 has been totally disabled
Node 432 has been totally disabled
Node 433 has been totally disabled
Node 434 has been totally disabled
Node 435 has been totally disabled
Node 436 has been totally disabled
Node 437 has been totally disabled
Node 438 has been totally disabled
Node 439 has been totally disabled
Node 440 has been totally disabled
Node 441 has been totally disabled
Node 442 has been totally disabled
Node 443 has been totally disabled
Node 444 has been totally disabled
Node 445 has been totally disabled
Node 446 has been totally disabled
Node 447 has been totally disabled
Node 448 has been totally disabled
Node 449 has been totally disabled
Node 450 has been totally disabled
Node 451 has been totally disabled
Node 452 has been totally disabled
Node 453 has been totally disabled
Node 454 has been totally disabled
Node 455 has been totally disabled
Node 456 has been totally disabled
Node 457 has been totally disabled
Node 458 has been totally disabled
Node 459 has been totally disabled
Node 460 has been totally disabled
Node 461 has been totally disabled
Node 462 has been totally disabled
Node 463 has been totally disabled
Node 464 has been totally disabled
Node 465 has been totally disabled
Node 466 has been totally disabled
Node 467 has been totally disabled
Node 468 has been totally disabled
Node 469 has been totally disabled
Node 470 has been totally disabled
Node 471 has been totally disabled
Node 472 has been totally disabled
Node 473 has been totally disabled
Node 474 has been totally disabled
Node 475 has been totally disabled
Node 476 has been totally disabled
Node 477 has been totally disabled
Node 478 has been totally disabled
Node 479 has been totally disabled
Node 480 has been totally disabled
Node 481 has been totally disabled
Node 482 has been totally disabled
Node 483 has been totally disabled
Node 484 has been totally disabled
Node 485 has been totally disabled
Node 486 has been totally disabled
Node 487 has been totally disabled
Node 488 has been totally disabled
Node 489 has been totally disabled
Node 490 has been totally disabled
Node 491 has been totally disabled
Node 492 has been totally disabled
Node 493 has been totally disabled
Node 494 has been totally disabled
Node 495 has been totally disabled
Node 496 has been totally disabled
Node 497 has been totally disabled
Node 498 has been totally disabled
Node 499 has been totally disabled
Node 500 has been totally disabled
Node 501 has been totally disabled
Node 502 has been totally disabled
Node 503 has been totally disabled
Node 504 has been totally disabled
Node 505 has been totally disabled
Node 506 has been totally disabled
Node 507 has been totally disabled
Node 508 has been totally disabled
Node 509 has been totally disabled
Node 510 has been totally disabled
Node 511 has been totally disabled
Node 513 has been totally disabled
Node 514 has been totally disabled
Node 515 has been totally disabled
Node 516 has been totally disabled
Node 517 has been totally disabled
Node 518 has been totally disabled
Node 519 has been totally disabled
Node 520 has been totally disabled
Node 521 has been totally disabled
Node 522 has been totally disabled
Node 523 has been totally disabled
Node 524 has been totally disabled
Node 525 has been totally disabled
Node 526 has been totally disabled
Node 527 has been totally disabled
Node 528 has been totally disabled
Node 529 has been totally disabled
Node 530 has been totally disabled
Node 531 has been totally disabled
Node 532 has been totally disabled
Node 533 has been totally disabled
Node 534 has been totally disabled
Node 535 has been totally disabled
Node 536 has been totally disabled
Node 537 has been totally disabled
Node 538 has been totally disabled
Node 539 has been totally disabled
Node 540 has been totally disabled
Node 541 has been totally disabled
Node 542 has been totally disabled
Node 543 has been totally disabled
Node 544 has been totally disabled
Node 545 has been totally disabled
Node 546 has been totally disabled
Node 547 has been totally disabled
Node 548 has been totally disabled
Node 549 has been totally disabled
Node 550 has been totally disabled
Node 551 has been totally disabled
Node 552 has been totally disabled
Node 553 has been totally disabled
Node 554 has been totally disabled
Node 555 has been totally disabled
Node 556 has been totally disabled
Node 557 has been totally disabled
Node 558 has been totally disabled
Node 559 has been totally disabled
Node 560 has been totally disabled
Node 561 has been totally disabled
Node 562 has been totally disabled
Node 563 has been totally disabled
Node 564 has been totally disabled
Node 565 has been totally disabled
Node 566 has been totally disabled
Node 567 has been totally disabled
Node 568 has been totally disabled
Node 569 has been totally disabled
Node 570 has been totally disabled
Node 571 has been totally disabled
Node 572 has been totally disabled
Node 573 has been totally disabled
Node 574 has been totally disabled
Node 575 has been totally disabled
Node 576 has been totally disabled
Node 577 has been totally disabled
Node 578 has been totally disabled
Node 579 has been totally disabled
Node 580 has been totally disabled
Node 581 has been totally disabled
Node 582 has been totally disabled
Node 583 has been totally disabled
Node 584 has been totally disabled
Node 585 has been totally disabled
Node 586 has been totally disabled
Node 587 has been totally disabled
Node 588 has been totally disabled
Node 589 has been totally disabled
Node 590 has been totally disabled
Node 591 has been totally disabled
Node 592 has been totally disabled
Node 593 has been totally disabled
Node 594 has been totally disabled
Node 595 has been totally disabled
Node 596 has been totally disabled
Node 597 has been totally disabled
Node 598 has been totally disabled
Node 599 has been totally disabled
Node 600 has been totally disabled
Node 601 has been totally disabled
Node 602 has been totally disabled
Node 603 has been totally disabled
Node 604 has been totally disabled
Node 605 has been totally disabled
Node 606 has been totally disabled
Node 607 has been totally disabled
Node 608 has been totally disabled
Node 609 has been totally disabled
Node 610 has been totally disabled
Node 611 has been totally disabled
Node 612 has been totally disabled
Node 613 has been totally disabled
Node 614 has been totally disabled
Node 615 has been totally disabled
Node 616 has been totally disabled
Node 617 has been totally disabled
Node 618 has been totally disabled
Node 619 has been totally disabled
Node 620 has been totally disabled
Node 621 has been totally disabled
Node 622 has been totally disabled
Node 623 has been totally disabled
Node 624 has been totally disabled
Node 625 has been totally disabled
Node 626 has been totally disabled
Node 627 has been totally disabled
Node 628 has been totally disabled
Node 629 has been totally disabled
Node 630 has been totally disabled
Node 631 has been totally disabled
Node 632 has been totally disabled
Node 633 has been totally disabled
Node 634 has been totally disabled
Node 635 has been totally disabled
Node 636 has been totally disabled
Node 637 has been totally disabled
Node 638 has been totally disabled
Node 639 has been totally disabled
Node 641 has been totally disabled
Node 642 has been totally disabled
Node 643 has been totally disabled
Node 644 has been totally disabled
Node 645 has been totally disabled
Node 646 has been totally disabled
Node 647 has been totally disabled
Node 648 has been totally disabled
Node 649 has been totally disabled
Node 650 has been totally disabled
Node 651 has been totally disabled
Node 652 has been totally disabled
Node 653 has been totally disabled
Node 654 has been totally disabled
Node 655 has been totally disabled
Node 656 has been totally disabled
Node 657 has been totally disabled
Node 658 has been totally disabled
Node 659 has been totally disabled
Node 660 has been totally disabled
Node 661 has been totally disabled
Node 662 has been totally disabled
Node 663 has been totally disabled
Node 664 has been totally disabled
Node 665 has been totally disabled
Node 666 has been totally disabled
Node 667 has been totally disabled
Node 668 has been totally disabled
Node 669 has been totally disabled
Node 670 has been totally disabled
Node 671 has been totally disabled
Node 672 has been totally disabled
Node 673 has been totally disabled
Node 674 has been totally disabled
Node 675 has been totally disabled
Node 676 has been totally disabled
Node 677 has been totally disabled
Node 678 has been totally disabled
Node 679 has been totally disabled
Node 680 has been totally disabled
Node 681 has been totally disabled
Node 682 has been totally disabled
Node 683 has been totally disabled
Node 684 has been totally disabled
Node 685 has been totally disabled
Node 686 has been totally disabled
Node 687 has been totally disabled
Node 688 has been totally disabled
Node 689 has been totally disabled
Node 690 has been totally disabled
Node 691 has been totally disabled
Node 692 has been totally disabled
Node 693 has been totally disabled
Node 694 has been totally disabled
Node 695 has been totally disabled
Node 696 has been totally disabled
Node 697 has been totally disabled
Node 698 has been totally disabled
Node 699 has been totally disabled
Node 700 has been totally disabled
Node 701 has been totally disabled
Node 702 has been totally disabled
Node 703 has been totally disabled
Node 704 has been totally disabled
Node 705 has been totally disabled
Node 706 has been totally disabled
Node 707 has been totally disabled
Node 708 has been totally disabled
Node 709 has been totally disabled
Node 710 has been totally disabled
Node 711 has been totally disabled
Node 712 has been totally disabled
Node 713 has been totally disabled
Node 714 has been totally disabled
Node 715 has been totally disabled
Node 716 has been totally disabled
Node 717 has been totally disabled
Node 718 has been totally disabled
Node 719 has been totally disabled
Node 720 has been totally disabled
Node 721 has been totally disabled
Node 722 has been totally disabled
Node 723 has been totally disabled
Node 724 has been totally disabled
Node 725 has been totally disabled
Node 726 has been totally disabled
Node 727 has been totally disabled
Node 728 has been totally disabled
Node 729 has been totally disabled
Node 730 has been totally disabled
Node 731 has been totally disabled
Node 732 has been totally disabled
Node 733 has been totally disabled
Node 734 has been totally disabled
Node 735 has been totally disabled
Node 736 has been totally disabled
Node 737 has been totally disabled
Node 738 has been totally disabled
Node 739 has been totally disabled
Node 740 has been totally disabled
Node 741 has been totally disabled
Node 742 has been totally disabled
Node 743 has been totally disabled
Node 744 has been totally disabled
Node 745 has been totally disabled
Node 746 has been totally disabled
Node 747 has been totally disabled
Node 748 has been totally disabled
Node 749 has been totally disabled
Node 750 has been totally disabled
Node 751 has been totally disabled
Node 752 has been totally disabled
Node 753 has been totally disabled
Node 754 has been totally disabled
Node 755 has been totally disabled
Node 756 has been totally disabled
Node 757 has been totally disabled
Node 758 has been totally disabled
Node 759 has been totally disabled
Node 760 has been totally disabled
Node 761 has been totally disabled
Node 762 has been totally disabled
Node 763 has been totally disabled
Node 764 has been totally disabled
Node 765 has been totally disabled
Node 766 has been totally disabled
Node 767 has been totally disabled
Node 769 has been totally disabled
Node 770 has been totally disabled
Node 771 has been totally disabled
Node 772 has been totally disabled
Node 773 has been totally disabled
Node 774 has been totally disabled
Node 775 has been totally disabled
Node 776 has been totally disabled
Node 777 has been totally disabled
Node 778 has been totally disabled
Node 779 has been totally disabled
Node 780 has been totally disabled
Node 781 has been totally disabled
Node 782 has been totally disabled
Node 783 has been totally disabled
Node 784 has been totally disabled
Node 785 has been totally disabled
Node 786 has been totally disabled
Node 787 has been totally disabled
Node 788 has been totally disabled
Node 789 has been totally disabled
Node 790 has been totally disabled
Node 791 has been totally disabled
Node 792 has been totally disabled
Node 793 has been totally disabled
Node 794 has been totally disabled
Node 795 has been totally disabled
Node 796 has been totally disabled
Node 797 has been totally disabled
Node 798 has been totally disabled
Node 799 has been totally disabled
Node 800 has been totally disabled
Node 801 has been totally disabled
Node 802 has been totally disabled
Node 803 has been totally disabled
Node 804 has been totally disabled
Node 805 has been totally disabled
Node 806 has been totally disabled
Node 807 has been totally disabled
Node 808 has been totally disabled
Node 809 has been totally disabled
Node 810 has been totally disabled
Node 811 has been totally disabled
Node 812 has been totally disabled
Node 813 has been totally disabled
Node 814 has been totally disabled
Node 815 has been totally disabled
Node 816 has been totally disabled
Node 817 has been totally disabled
Node 818 has been totally disabled
Node 819 has been totally disabled
Node 820 has been totally disabled
Node 821 has been totally disabled
Node 822 has been totally disabled
Node 823 has been totally disabled
Node 824 has been totally disabled
Node 825 has been totally disabled
Node 826 has been totally disabled
Node 827 has been totally disabled
Node 828 has been totally disabled
Node 829 has been totally disabled
Node 830 has been totally disabled
Node 831 has been totally disabled
Node 832 has been totally disabled
Node 833 has been totally disabled
Node 834 has been totally disabled
Node 835 has been totally disabled
Node 836 has been totally disabled
Node 837 has been totally disabled
Node 838 has been totally disabled
Node 839 has been totally disabled
Node 840 has been totally disabled
Node 841 has been totally disabled
Node 842 has been totally disabled
Node 843 has been totally disabled
Node 844 has been totally disabled
Node 845 has been totally disabled
Node 846 has been totally disabled
Node 847 has been totally disabled
Node 848 has been totally disabled
Node 849 has been totally disabled
Node 850 has been totally disabled
Node 851 has been totally disabled
Node 852 has been totally disabled
Node 853 has been totally disabled
Node 854 has been totally disabled
Node 855 has been totally disabled
Node 856 has been totally disabled
Node 857 has been totally disabled
Node 858 has been totally disabled
Node 859 has been totally disabled
Node 860 has been totally disabled
Node 861 has been totally disabled
Node 862 has been totally disabled
Node 863 has been totally disabled
Node 864 has been totally disabled
Node 865 has been totally disabled
Node 866 has been totally disabled
Node 867 has been totally disabled
Node 868 has been totally disabled
Node 869 has been totally disabled
Node 870 has been totally disabled
Node 871 has been totally disabled
Node 872 has been totally disabled
Node 873 has been totally disabled
Node 874 has been totally disabled
Node 875 has been totally disabled
Node 876 has been totally disabled
Node 877 has been totally disabled
Node 878 has been totally disabled
Node 879 has been totally disabled
Node 880 has been totally disabled
Node 881 has been totally disabled
Node 882 has been totally disabled
Node 883 has been totally disabled
Node 884 has been totally disabled
Node 885 has been totally disabled
Node 886 has been totally disabled
Node 887 has been totally disabled
Node 888 has been totally disabled
Node 889 has been totally disabled
Node 890 has been totally disabled
Node 891 has been totally disabled
Node 892 has been totally disabled
Node 893 has been totally disabled
Node 894 has been totally disabled
Node 895 has been totally disabled
Node 897 has been totally disabled
Node 898 has been totally disabled
Node 899 has been totally disabled
Node 900 has been totally disabled
Node 901 has been totally disabled
Node 902 has been totally disabled
Node 903 has been totally disabled
Node 904 has been totally disabled
Node 905 has been totally disabled
Node 906 has been totally disabled
Node 907 has been totally disabled
Node 908 has been totally disabled
Node 909 has been totally disabled
Node 910 has been totally disabled
Node 911 has been totally disabled
Node 912 has been totally disabled
Node 913 has been totally disabled
Node 914 has been totally disabled
Node 915 has been totally disabled
Node 916 has been totally disabled
Node 917 has been totally disabled
Node 918 has been totally disabled
Node 919 has been totally disabled
Node 920 has been totally disabled
Node 921 has been totally disabled
Node 922 has been totally disabled
Node 923 has been totally disabled
Node 924 has been totally disabled
Node 925 has been totally disabled
Node 926 has been totally disabled
Node 927 has been totally disabled
Node 928 has been totally disabled
Node 929 has been totally disabled
Node 930 has been totally disabled
Node 931 has been totally disabled
Node 932 has been totally disabled
Node 933 has been totally disabled
Node 934 has been totally disabled
Node 935 has been totally disabled
Node 936 has been totally disabled
Node 937 has been totally disabled
Node 938 has been totally disabled
Node 939 has been totally disabled
Node 940 has been totally disabled
Node 941 has been totally disabled
Node 942 has been totally disabled
Node 943 has been totally disabled
Node 944 has been totally disabled
Node 945 has been totally disabled
Node 946 has been totally disabled
Node 947 has been totally disabled
Node 948 has been totally disabled
Node 949 has been totally disabled
Node 950 has been totally disabled
Node 951 has been totally disabled
Node 952 has been totally disabled
Node 953 has been totally disabled
Node 954 has been totally disabled
Node 955 has been totally disabled
Node 956 has been totally disabled
Node 957 has been totally disabled
Node 958 has been totally disabled
Node 959 has been totally disabled
Node 960 has been totally disabled
Node 961 has been totally disabled
Node 962 has been totally disabled
Node 963 has been totally disabled
Node 964 has been totally disabled
Node 965 has been totally disabled
Node 966 has been totally disabled
Node 967 has been totally disabled
Node 968 has been totally disabled
Node 969 has been totally disabled
Node 970 has been totally disabled
Node 971 has been totally disabled
Node 972 has been totally disabled
Node 973 has been totally disabled
Node 974 has been totally disabled
Node 975 has been totally disabled
Node 976 has been totally disabled
Node 977 has been totally disabled
Node 978 has been totally disabled
Node 979 has been totally disabled
Node 980 has been totally disabled
Node 981 has been totally disabled
Node 982 has been totally disabled
Node 983 has been totally disabled
Node 984 has been totally disabled
Node 985 has been totally disabled
Node 986 has been totally disabled
Node 987 has been totally disabled
Node 988 has been totally disabled
Node 989 has been totally disabled
Node 990 has been totally disabled
Node 991 has been totally disabled
Node 992 has been totally disabled
Node 993 has been totally disabled
Node 994 has been totally disabled
Node 995 has been totally disabled
Node 996 has been totally disabled
Node 997 has been totally disabled
Node 998 has been totally disabled
Node 999 has been totally disabled
Node 1000 has been totally disabled
Node 1001 has been totally disabled
Node 1002 has been totally disabled
Node 1003 has been totally disabled
Node 1004 has been totally disabled
Node 1005 has been totally disabled
Node 1006 has been totally disabled
Node 1007 has been totally disabled
Node 1008 has been totally disabled
Node 1009 has been totally disabled
Node 1010 has been totally disabled
Node 1011 has been totally disabled
Node 1012 has been totally disabled
Node 1013 has been totally disabled
Node 1014 has been totally disabled
Node 1015 has been totally disabled
Node 1016 has been totally disabled
Node 1017 has been totally disabled
Node 1018 has been totally disabled
Node 1019 has been totally disabled
Node 1020 has been totally disabled
Node 1021 has been totally disabled
Node 1022 has been totally disabled
Node 1023 has been totally disabled
info: all-reduce weight grad collective issued for layer: fc1000 with size: 819200000, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_3_conv3 with size: 419430400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_3_conv2 with size: 943718400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_3_conv1 with size: 419430400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_2_conv3 with size: 419430400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_2_conv2 with size: 943718400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_2_conv1 with size: 419430400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_1_conv3 with size: 419430400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_1_conv2 with size: 943718400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_1_conv1 with size: 209715200, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_512_1_conv4 with size: 838860800, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_6_conv3 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_6_conv2 with size: 235929600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_6_conv1 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_5_conv3 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_5_conv2 with size: 235929600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_5_conv1 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_4_conv3 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_4_conv2 with size: 235929600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_4_conv1 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_3_conv3 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_3_conv2 with size: 235929600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_3_conv1 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_2_conv3 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_2_conv2 with size: 235929600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_2_conv1 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_1_conv3 with size: 104857600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_1_conv2 with size: 235929600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_1_conv1 with size: 52428800, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_256_1_conv4 with size: 209715200, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_4_conv3 with size: 26214400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_4_conv2 with size: 58982400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_4_conv1 with size: 26214400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_3_conv3 with size: 26214400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_3_conv2 with size: 58982400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_3_conv1 with size: 26214400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_2_conv3 with size: 26214400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_2_conv2 with size: 58982400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_2_conv1 with size: 26214400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_1_conv3 with size: 26214400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_1_conv2 with size: 58982400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_1_conv1 with size: 13107200, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_128_1_conv4 with size: 52428800, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_3_conv3 with size: 6553600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_3_conv2 with size: 14745600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_3_conv1 with size: 6553600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_2_conv3 with size: 6553600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_2_conv2 with size: 14745600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_2_conv1 with size: 6553600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_1_conv3 with size: 6553600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_1_conv2 with size: 14745600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_1_conv1 with size: 1638400, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: layer_64_1_conv4 with size: 6553600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-reduce weight grad collective issued for layer: conv1 with size: 3763200, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
pass: 0 finished at time: 281506
***** info: weight gradient collective for layer: layer_64_1_conv1 is finished************
***** info: weight gradient collective for layer: layer_64_3_conv3 is finished************
***** info: weight gradient collective for layer: conv1 is finished************
***** info: weight gradient collective for layer: layer_64_3_conv1 is finished************
***** info: weight gradient collective for layer: layer_64_2_conv3 is finished************
***** info: weight gradient collective for layer: layer_64_2_conv1 is finished************
***** info: weight gradient collective for layer: layer_64_1_conv3 is finished************
***** info: weight gradient collective for layer: layer_64_1_conv4 is finished************
***** info: weight gradient collective for layer: layer_128_1_conv1 is finished************
***** info: weight gradient collective for layer: layer_64_3_conv2 is finished************
***** info: weight gradient collective for layer: layer_64_2_conv2 is finished************
***** info: weight gradient collective for layer: layer_64_1_conv2 is finished************
***** info: weight gradient collective for layer: layer_128_4_conv3 is finished************
***** info: weight gradient collective for layer: layer_128_4_conv1 is finished************
***** info: weight gradient collective for layer: layer_128_3_conv3 is finished************
***** info: weight gradient collective for layer: layer_128_3_conv1 is finished************
***** info: weight gradient collective for layer: layer_128_2_conv3 is finished************
***** info: weight gradient collective for layer: layer_128_2_conv1 is finished************
***** info: weight gradient collective for layer: layer_128_1_conv3 is finished************
***** info: weight gradient collective for layer: layer_256_1_conv1 is finished************
***** info: weight gradient collective for layer: layer_128_1_conv4 is finished************
***** info: weight gradient collective for layer: layer_128_4_conv2 is finished************
***** info: weight gradient collective for layer: layer_128_3_conv2 is finished************
***** info: weight gradient collective for layer: layer_128_2_conv2 is finished************
***** info: weight gradient collective for layer: layer_128_1_conv2 is finished************
***** info: weight gradient collective for layer: layer_256_5_conv1 is finished************
***** info: weight gradient collective for layer: layer_256_6_conv1 is finished************
***** info: weight gradient collective for layer: layer_256_6_conv3 is finished************
***** info: weight gradient collective for layer: layer_256_4_conv1 is finished************
***** info: weight gradient collective for layer: layer_256_3_conv1 is finished************
***** info: weight gradient collective for layer: layer_256_2_conv1 is finished************
***** info: weight gradient collective for layer: layer_256_5_conv3 is finished************
***** info: weight gradient collective for layer: layer_256_4_conv3 is finished************
***** info: weight gradient collective for layer: layer_256_3_conv3 is finished************
***** info: weight gradient collective for layer: layer_256_2_conv3 is finished************
***** info: weight gradient collective for layer: layer_256_1_conv3 is finished************
***** info: weight gradient collective for layer: layer_512_1_conv1 is finished************
***** info: weight gradient collective for layer: layer_256_1_conv4 is finished************
***** info: weight gradient collective for layer: layer_256_6_conv2 is finished************
***** info: weight gradient collective for layer: layer_256_5_conv2 is finished************
***** info: weight gradient collective for layer: layer_256_4_conv2 is finished************
***** info: weight gradient collective for layer: layer_256_3_conv2 is finished************
***** info: weight gradient collective for layer: layer_256_2_conv2 is finished************
***** info: weight gradient collective for layer: layer_256_1_conv2 is finished************
***** info: weight gradient collective for layer: layer_512_3_conv3 is finished************
***** info: weight gradient collective for layer: layer_512_3_conv1 is finished************
***** info: weight gradient collective for layer: layer_512_2_conv1 is finished************
***** info: weight gradient collective for layer: layer_512_2_conv3 is finished************
***** info: weight gradient collective for layer: layer_512_1_conv3 is finished************
***** info: weight gradient collective for layer: fc1000 is finished************
***** info: weight gradient collective for layer: layer_512_1_conv4 is finished************
***** info: weight gradient collective for layer: layer_512_3_conv2 is finished************
***** info: weight gradient collective for layer: layer_512_2_conv2 is finished************
***** info: weight gradient collective for layer: layer_512_1_conv2 is finished************
*******************
Layer id: conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* conv1
id: conv1 ,Total cycles spent on fwd pass compute: 3380
id: conv1 ,Total cycles spent on weight grad compute: 8395
id: conv1 ,Total cycles spent on input grad compute: 0
id: conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: conv1 ,Total cycles spent idle waiting for weight grad finish: 29306299
id: conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: conv1 ,Total cycles spent on fwd pass comm: 0
id: conv1 ,Total cycles spent on weight grad comm: 29306300
id: conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* conv1
id: conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 15947.6
id: conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 7374.41
id: conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 6750.06
id: conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 18437.2
id: conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0.21875
id: conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 274.188
id: conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.19111e+07
id: conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 18844.3
*************************  Network stats  ************************* conv1
id: conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 96.6042
id: conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 33.6094
id: conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 8.33333
id: conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 3.76563
id: conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 3.76563
id: conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 8.15625
id: conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 32.862
id: conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 96
*******************
Layer id: layer_64_1_conv4
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_1_conv4
id: layer_64_1_conv4 ,Total cycles spent on fwd pass compute: 898
id: layer_64_1_conv4 ,Total cycles spent on weight grad compute: 1946
id: layer_64_1_conv4 ,Total cycles spent on input grad compute: 948
id: layer_64_1_conv4 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_1_conv4 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_1_conv4 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_1_conv4 ,Total cycles spent on fwd pass comm: 0
id: layer_64_1_conv4 ,Total cycles spent on weight grad comm: 29323227
id: layer_64_1_conv4 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_1_conv4
id: layer_64_1_conv4 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_1_conv4 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 19081.2
id: layer_64_1_conv4 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 13353.5
id: layer_64_1_conv4 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 13252.7
id: layer_64_1_conv4 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 21748.2
id: layer_64_1_conv4 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_1_conv4 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 308.703
id: layer_64_1_conv4 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17581e+07
id: layer_64_1_conv4 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 28494.7
*************************  Network stats  ************************* layer_64_1_conv4
id: layer_64_1_conv4 ,Average cycles spent on network for phase 1 of algorithm (per message): 166.547
id: layer_64_1_conv4 ,Average cycles spent on network for phase 2 of algorithm (per message): 57.1432
id: layer_64_1_conv4 ,Average cycles spent on network for phase 3 of algorithm (per message): 15.151
id: layer_64_1_conv4 ,Average cycles spent on network for phase 4 of algorithm (per message): 4.48958
id: layer_64_1_conv4 ,Average cycles spent on network for phase 5 of algorithm (per message): 4.48958
id: layer_64_1_conv4 ,Average cycles spent on network for phase 6 of algorithm (per message): 15.151
id: layer_64_1_conv4 ,Average cycles spent on network for phase 7 of algorithm (per message): 57.1432
id: layer_64_1_conv4 ,Average cycles spent on network for phase 8 of algorithm (per message): 166.547
*******************
Layer id: layer_64_1_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_1_conv1
id: layer_64_1_conv1 ,Total cycles spent on fwd pass compute: 898
id: layer_64_1_conv1 ,Total cycles spent on weight grad compute: 1946
id: layer_64_1_conv1 ,Total cycles spent on input grad compute: 898
id: layer_64_1_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_1_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_1_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_1_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_64_1_conv1 ,Total cycles spent on weight grad comm: 29315052
id: layer_64_1_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_1_conv1
id: layer_64_1_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_1_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 7166.59
id: layer_64_1_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 8187.06
id: layer_64_1_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 12341
id: layer_64_1_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 23112
id: layer_64_1_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_1_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 450.094
id: layer_64_1_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17689e+07
id: layer_64_1_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 22508.8
*************************  Network stats  ************************* layer_64_1_conv1
id: layer_64_1_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 42.125
id: layer_64_1_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 14.9792
id: layer_64_1_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 4.57031
id: layer_64_1_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 2.13542
id: layer_64_1_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 2.13542
id: layer_64_1_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 4.57031
id: layer_64_1_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 14.9792
id: layer_64_1_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 42.125
*******************
Layer id: layer_64_1_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_1_conv2
id: layer_64_1_conv2 ,Total cycles spent on fwd pass compute: 2845
id: layer_64_1_conv2 ,Total cycles spent on weight grad compute: 3677
id: layer_64_1_conv2 ,Total cycles spent on input grad compute: 2695
id: layer_64_1_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_1_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_1_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_1_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_64_1_conv2 ,Total cycles spent on weight grad comm: 29346016
id: layer_64_1_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_1_conv2
id: layer_64_1_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_1_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 96651.2
id: layer_64_1_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 17977.2
id: layer_64_1_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 8175.5
id: layer_64_1_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 221.969
id: layer_64_1_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_1_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 589.609
id: layer_64_1_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17329e+07
id: layer_64_1_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 45659.3
*************************  Network stats  ************************* layer_64_1_conv2
id: layer_64_1_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 373.187
id: layer_64_1_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 127.609
id: layer_64_1_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 33.0104
id: layer_64_1_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 8.88542
id: layer_64_1_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 8.88542
id: layer_64_1_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 33.0104
id: layer_64_1_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 127.609
id: layer_64_1_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 373.187
*******************
Layer id: layer_64_1_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_1_conv3
id: layer_64_1_conv3 ,Total cycles spent on fwd pass compute: 898
id: layer_64_1_conv3 ,Total cycles spent on weight grad compute: 1946
id: layer_64_1_conv3 ,Total cycles spent on input grad compute: 948
id: layer_64_1_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_1_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_1_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_1_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_64_1_conv3 ,Total cycles spent on weight grad comm: 29333666
id: layer_64_1_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_1_conv3
id: layer_64_1_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_1_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 21888.7
id: layer_64_1_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 15591.3
id: layer_64_1_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 16912.9
id: layer_64_1_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 28538.3
id: layer_64_1_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_1_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 314.031
id: layer_64_1_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17562e+07
id: layer_64_1_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 26714.1
*************************  Network stats  ************************* layer_64_1_conv3
id: layer_64_1_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 166.547
id: layer_64_1_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 57.1432
id: layer_64_1_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 15.151
id: layer_64_1_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 4.48958
id: layer_64_1_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 4.48958
id: layer_64_1_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 15.151
id: layer_64_1_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 57.1432
id: layer_64_1_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 166.547
*******************
Layer id: layer_64_2_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_2_conv1
id: layer_64_2_conv1 ,Total cycles spent on fwd pass compute: 948
id: layer_64_2_conv1 ,Total cycles spent on weight grad compute: 2595
id: layer_64_2_conv1 ,Total cycles spent on input grad compute: 898
id: layer_64_2_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_2_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_2_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_2_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_64_2_conv1 ,Total cycles spent on weight grad comm: 29335084
id: layer_64_2_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_2_conv1
id: layer_64_2_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_2_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 41238.5
id: layer_64_2_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 14260.3
id: layer_64_2_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 13512.4
id: layer_64_2_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 15047
id: layer_64_2_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_2_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 381.031
id: layer_64_2_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17557e+07
id: layer_64_2_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 28622
*************************  Network stats  ************************* layer_64_2_conv1
id: layer_64_2_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 166.547
id: layer_64_2_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 57.1432
id: layer_64_2_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 15.151
id: layer_64_2_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 4.48958
id: layer_64_2_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 4.48958
id: layer_64_2_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 15.151
id: layer_64_2_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 57.1432
id: layer_64_2_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 166.547
*******************
Layer id: layer_64_2_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_2_conv2
id: layer_64_2_conv2 ,Total cycles spent on fwd pass compute: 2845
id: layer_64_2_conv2 ,Total cycles spent on weight grad compute: 3677
id: layer_64_2_conv2 ,Total cycles spent on input grad compute: 2695
id: layer_64_2_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_2_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_2_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_2_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_64_2_conv2 ,Total cycles spent on weight grad comm: 29355018
id: layer_64_2_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_2_conv2
id: layer_64_2_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_2_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 133610
id: layer_64_2_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 14087.9
id: layer_64_2_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 1709.77
id: layer_64_2_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 234.844
id: layer_64_2_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_2_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 485.406
id: layer_64_2_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17232e+07
id: layer_64_2_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 44035
*************************  Network stats  ************************* layer_64_2_conv2
id: layer_64_2_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 373.187
id: layer_64_2_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 127.609
id: layer_64_2_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 33.0104
id: layer_64_2_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 8.88542
id: layer_64_2_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 8.88542
id: layer_64_2_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 33.0104
id: layer_64_2_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 127.609
id: layer_64_2_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 373.187
*******************
Layer id: layer_64_2_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_2_conv3
id: layer_64_2_conv3 ,Total cycles spent on fwd pass compute: 898
id: layer_64_2_conv3 ,Total cycles spent on weight grad compute: 1946
id: layer_64_2_conv3 ,Total cycles spent on input grad compute: 948
id: layer_64_2_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_2_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_2_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_2_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_64_2_conv3 ,Total cycles spent on weight grad comm: 29343077
id: layer_64_2_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_2_conv3
id: layer_64_2_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_2_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 23004.9
id: layer_64_2_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 17962.8
id: layer_64_2_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 20985.1
id: layer_64_2_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 37613.7
id: layer_64_2_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_2_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 325.219
id: layer_64_2_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.1755e+07
id: layer_64_2_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 24944
*************************  Network stats  ************************* layer_64_2_conv3
id: layer_64_2_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 166.547
id: layer_64_2_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 57.1432
id: layer_64_2_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 15.151
id: layer_64_2_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 4.48958
id: layer_64_2_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 4.48958
id: layer_64_2_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 15.151
id: layer_64_2_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 57.1432
id: layer_64_2_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 166.547
*******************
Layer id: layer_64_3_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_3_conv1
id: layer_64_3_conv1 ,Total cycles spent on fwd pass compute: 948
id: layer_64_3_conv1 ,Total cycles spent on weight grad compute: 2595
id: layer_64_3_conv1 ,Total cycles spent on input grad compute: 898
id: layer_64_3_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_3_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_3_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_3_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_64_3_conv1 ,Total cycles spent on weight grad comm: 29344495
id: layer_64_3_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_3_conv1
id: layer_64_3_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_3_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 56736.9
id: layer_64_3_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 15274.2
id: layer_64_3_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 14326
id: layer_64_3_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 12402.8
id: layer_64_3_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_3_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 443.844
id: layer_64_3_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17537e+07
id: layer_64_3_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 29061.7
*************************  Network stats  ************************* layer_64_3_conv1
id: layer_64_3_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 166.547
id: layer_64_3_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 57.1432
id: layer_64_3_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 15.151
id: layer_64_3_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 4.48958
id: layer_64_3_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 4.48958
id: layer_64_3_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 15.151
id: layer_64_3_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 57.1432
id: layer_64_3_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 166.547
*******************
Layer id: layer_64_3_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_3_conv2
id: layer_64_3_conv2 ,Total cycles spent on fwd pass compute: 2845
id: layer_64_3_conv2 ,Total cycles spent on weight grad compute: 3677
id: layer_64_3_conv2 ,Total cycles spent on input grad compute: 2695
id: layer_64_3_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_3_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_3_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_3_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_64_3_conv2 ,Total cycles spent on weight grad comm: 29364020
id: layer_64_3_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_3_conv2
id: layer_64_3_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_3_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 172625
id: layer_64_3_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 9703.27
id: layer_64_3_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 561.516
id: layer_64_3_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 452.172
id: layer_64_3_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_3_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 487.766
id: layer_64_3_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17076e+07
id: layer_64_3_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 35546.8
*************************  Network stats  ************************* layer_64_3_conv2
id: layer_64_3_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 373.187
id: layer_64_3_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 127.609
id: layer_64_3_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 33.0104
id: layer_64_3_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 8.88542
id: layer_64_3_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 8.88542
id: layer_64_3_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 33.0104
id: layer_64_3_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 127.609
id: layer_64_3_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 373.187
*******************
Layer id: layer_64_3_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_64_3_conv3
id: layer_64_3_conv3 ,Total cycles spent on fwd pass compute: 898
id: layer_64_3_conv3 ,Total cycles spent on weight grad compute: 1946
id: layer_64_3_conv3 ,Total cycles spent on input grad compute: 948
id: layer_64_3_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_64_3_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_64_3_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_64_3_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_64_3_conv3 ,Total cycles spent on weight grad comm: 29352488
id: layer_64_3_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_64_3_conv3
id: layer_64_3_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_64_3_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 28621.8
id: layer_64_3_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 20313
id: layer_64_3_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 24923.4
id: layer_64_3_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 42201.7
id: layer_64_3_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_64_3_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 419.719
id: layer_64_3_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17526e+07
id: layer_64_3_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 22839.4
*************************  Network stats  ************************* layer_64_3_conv3
id: layer_64_3_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 166.547
id: layer_64_3_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 57.1432
id: layer_64_3_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 15.151
id: layer_64_3_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 4.48958
id: layer_64_3_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 4.48958
id: layer_64_3_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 15.151
id: layer_64_3_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 57.1432
id: layer_64_3_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 166.547
*******************
Layer id: layer_128_1_conv4
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_1_conv4
id: layer_128_1_conv4 ,Total cycles spent on fwd pass compute: 673
id: layer_128_1_conv4 ,Total cycles spent on weight grad compute: 1597
id: layer_128_1_conv4 ,Total cycles spent on input grad compute: 673
id: layer_128_1_conv4 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_1_conv4 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_1_conv4 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_1_conv4 ,Total cycles spent on fwd pass comm: 0
id: layer_128_1_conv4 ,Total cycles spent on weight grad comm: 29451502
id: layer_128_1_conv4 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_1_conv4
id: layer_128_1_conv4 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_1_conv4 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 519050
id: layer_128_1_conv4 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 7935.89
id: layer_128_1_conv4 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 1276.12
id: layer_128_1_conv4 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 1187.8
id: layer_128_1_conv4 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_1_conv4 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 1725.38
id: layer_128_1_conv4 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.14582e+07
id: layer_128_1_conv4 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 118505
*************************  Network stats  ************************* layer_128_1_conv4
id: layer_128_1_conv4 ,Average cycles spent on network for phase 1 of algorithm (per message): 1324.02
id: layer_128_1_conv4 ,Average cycles spent on network for phase 2 of algorithm (per message): 449.701
id: layer_128_1_conv4 ,Average cycles spent on network for phase 3 of algorithm (per message): 113.865
id: layer_128_1_conv4 ,Average cycles spent on network for phase 4 of algorithm (per message): 27.25
id: layer_128_1_conv4 ,Average cycles spent on network for phase 5 of algorithm (per message): 27.25
id: layer_128_1_conv4 ,Average cycles spent on network for phase 6 of algorithm (per message): 113.865
id: layer_128_1_conv4 ,Average cycles spent on network for phase 7 of algorithm (per message): 449.701
id: layer_128_1_conv4 ,Average cycles spent on network for phase 8 of algorithm (per message): 1324.02
*******************
Layer id: layer_128_1_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_1_conv1
id: layer_128_1_conv1 ,Total cycles spent on fwd pass compute: 948
id: layer_128_1_conv1 ,Total cycles spent on weight grad compute: 2595
id: layer_128_1_conv1 ,Total cycles spent on input grad compute: 915
id: layer_128_1_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_1_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_1_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_1_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_128_1_conv1 ,Total cycles spent on weight grad comm: 29369603
id: layer_128_1_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_1_conv1
id: layer_128_1_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_1_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 88430.6
id: layer_128_1_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 23138.9
id: layer_128_1_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 16109.5
id: layer_128_1_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 9002.3
id: layer_128_1_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_1_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 534.188
id: layer_128_1_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.17436e+07
id: layer_128_1_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 34322.9
*************************  Network stats  ************************* layer_128_1_conv1
id: layer_128_1_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 331.641
id: layer_128_1_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 113.365
id: layer_128_1_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 29.2526
id: layer_128_1_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 7.75
id: layer_128_1_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 7.75
id: layer_128_1_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 29.2526
id: layer_128_1_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 113.365
id: layer_128_1_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 331.641
*******************
Layer id: layer_128_1_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_1_conv2
id: layer_128_1_conv2 ,Total cycles spent on fwd pass compute: 1684
id: layer_128_1_conv2 ,Total cycles spent on weight grad compute: 1730
id: layer_128_1_conv2 ,Total cycles spent on input grad compute: 1518
id: layer_128_1_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_1_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_1_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_1_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_128_1_conv2 ,Total cycles spent on weight grad comm: 29518299
id: layer_128_1_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_1_conv2
id: layer_128_1_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_1_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 700893
id: layer_128_1_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 3348.62
id: layer_128_1_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 1627.39
id: layer_128_1_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 1659.19
id: layer_128_1_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_1_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 1961.39
id: layer_128_1_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.13345e+07
id: layer_128_1_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 196452
*************************  Network stats  ************************* layer_128_1_conv2
id: layer_128_1_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 1489.59
id: layer_128_1_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 505.922
id: layer_128_1_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 128.203
id: layer_128_1_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 30.8021
id: layer_128_1_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 30.8021
id: layer_128_1_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 128.203
id: layer_128_1_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 505.922
id: layer_128_1_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 1489.59
*******************
Layer id: layer_128_1_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_1_conv3
id: layer_128_1_conv3 ,Total cycles spent on fwd pass compute: 607
id: layer_128_1_conv3 ,Total cycles spent on weight grad compute: 1331
id: layer_128_1_conv3 ,Total cycles spent on input grad compute: 673
id: layer_128_1_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_1_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_1_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_1_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_128_1_conv3 ,Total cycles spent on weight grad comm: 29433714
id: layer_128_1_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_1_conv3
id: layer_128_1_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_1_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 188060
id: layer_128_1_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 28280.7
id: layer_128_1_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 14758.3
id: layer_128_1_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 3398.27
id: layer_128_1_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 1.1875
id: layer_128_1_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 813.719
id: layer_128_1_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.16841e+07
id: layer_128_1_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 101976
*************************  Network stats  ************************* layer_128_1_conv3
id: layer_128_1_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 662.531
id: layer_128_1_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 225.578
id: layer_128_1_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 57.4557
id: layer_128_1_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 14.2708
id: layer_128_1_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 14.2708
id: layer_128_1_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 57.4557
id: layer_128_1_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 225.578
id: layer_128_1_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 662.531
*******************
Layer id: layer_128_2_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_2_conv1
id: layer_128_2_conv1 ,Total cycles spent on fwd pass compute: 673
id: layer_128_2_conv1 ,Total cycles spent on weight grad compute: 1064
id: layer_128_2_conv1 ,Total cycles spent on input grad compute: 607
id: layer_128_2_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_2_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_2_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_2_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_128_2_conv1 ,Total cycles spent on weight grad comm: 29428868
id: layer_128_2_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_2_conv1
id: layer_128_2_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_2_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 244972
id: layer_128_2_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 14801.1
id: layer_128_2_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 6515.56
id: layer_128_2_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 2089.61
id: layer_128_2_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 1.1875
id: layer_128_2_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 748.312
id: layer_128_2_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.16606e+07
id: layer_128_2_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 87654.8
*************************  Network stats  ************************* layer_128_2_conv1
id: layer_128_2_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 662.531
id: layer_128_2_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 225.578
id: layer_128_2_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 57.4557
id: layer_128_2_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 14.2708
id: layer_128_2_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 14.2708
id: layer_128_2_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 57.4557
id: layer_128_2_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 225.578
id: layer_128_2_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 662.531
*******************
Layer id: layer_128_2_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_2_conv2
id: layer_128_2_conv2 ,Total cycles spent on fwd pass compute: 1684
id: layer_128_2_conv2 ,Total cycles spent on weight grad compute: 1730
id: layer_128_2_conv2 ,Total cycles spent on input grad compute: 1518
id: layer_128_2_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_2_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_2_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_2_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_128_2_conv2 ,Total cycles spent on weight grad comm: 29510199
id: layer_128_2_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_2_conv2
id: layer_128_2_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_2_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 804415
id: layer_128_2_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 3185.25
id: layer_128_2_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 2117.86
id: layer_128_2_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 1782.47
id: layer_128_2_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_2_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 2389.17
id: layer_128_2_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.12612e+07
id: layer_128_2_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 160825
*************************  Network stats  ************************* layer_128_2_conv2
id: layer_128_2_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 1489.59
id: layer_128_2_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 505.922
id: layer_128_2_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 128.203
id: layer_128_2_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 30.8021
id: layer_128_2_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 30.8021
id: layer_128_2_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 128.203
id: layer_128_2_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 505.922
id: layer_128_2_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 1489.59
*******************
Layer id: layer_128_2_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_2_conv3
id: layer_128_2_conv3 ,Total cycles spent on fwd pass compute: 607
id: layer_128_2_conv3 ,Total cycles spent on weight grad compute: 1331
id: layer_128_2_conv3 ,Total cycles spent on input grad compute: 673
id: layer_128_2_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_2_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_2_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_2_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_128_2_conv3 ,Total cycles spent on weight grad comm: 29427275
id: layer_128_2_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_2_conv3
id: layer_128_2_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_2_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 274827
id: layer_128_2_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 9621.72
id: layer_128_2_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 13007.5
id: layer_128_2_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 3898.61
id: layer_128_2_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_2_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 850.25
id: layer_128_2_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.1639e+07
id: layer_128_2_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 83262.5
*************************  Network stats  ************************* layer_128_2_conv3
id: layer_128_2_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 662.531
id: layer_128_2_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 225.578
id: layer_128_2_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 57.4557
id: layer_128_2_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 14.2708
id: layer_128_2_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 14.2708
id: layer_128_2_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 57.4557
id: layer_128_2_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 225.578
id: layer_128_2_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 662.531
*******************
Layer id: layer_128_3_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_3_conv1
id: layer_128_3_conv1 ,Total cycles spent on fwd pass compute: 673
id: layer_128_3_conv1 ,Total cycles spent on weight grad compute: 1064
id: layer_128_3_conv1 ,Total cycles spent on input grad compute: 607
id: layer_128_3_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_3_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_3_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_3_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_128_3_conv1 ,Total cycles spent on weight grad comm: 29422429
id: layer_128_3_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_3_conv1
id: layer_128_3_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_3_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 333119
id: layer_128_3_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 8286.22
id: layer_128_3_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 7339.17
id: layer_128_3_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 890.297
id: layer_128_3_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_3_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 809.547
id: layer_128_3_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.16048e+07
id: layer_128_3_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 77123.6
*************************  Network stats  ************************* layer_128_3_conv1
id: layer_128_3_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 662.531
id: layer_128_3_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 225.578
id: layer_128_3_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 57.4557
id: layer_128_3_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 14.2708
id: layer_128_3_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 14.2708
id: layer_128_3_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 57.4557
id: layer_128_3_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 225.578
id: layer_128_3_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 662.531
*******************
Layer id: layer_128_3_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_3_conv2
id: layer_128_3_conv2 ,Total cycles spent on fwd pass compute: 1684
id: layer_128_3_conv2 ,Total cycles spent on weight grad compute: 1730
id: layer_128_3_conv2 ,Total cycles spent on input grad compute: 1518
id: layer_128_3_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_3_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_3_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_3_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_128_3_conv2 ,Total cycles spent on weight grad comm: 29502099
id: layer_128_3_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_3_conv2
id: layer_128_3_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_3_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 907984
id: layer_128_3_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 3290.39
id: layer_128_3_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 2313.55
id: layer_128_3_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 1879.45
id: layer_128_3_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_3_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 2753.34
id: layer_128_3_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.11882e+07
id: layer_128_3_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 125417
*************************  Network stats  ************************* layer_128_3_conv2
id: layer_128_3_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 1489.59
id: layer_128_3_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 505.922
id: layer_128_3_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 128.203
id: layer_128_3_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 30.8021
id: layer_128_3_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 30.8021
id: layer_128_3_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 128.203
id: layer_128_3_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 505.922
id: layer_128_3_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 1489.59
*******************
Layer id: layer_128_3_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_3_conv3
id: layer_128_3_conv3 ,Total cycles spent on fwd pass compute: 607
id: layer_128_3_conv3 ,Total cycles spent on weight grad compute: 1331
id: layer_128_3_conv3 ,Total cycles spent on input grad compute: 673
id: layer_128_3_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_3_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_3_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_3_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_128_3_conv3 ,Total cycles spent on weight grad comm: 29420836
id: layer_128_3_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_3_conv3
id: layer_128_3_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_3_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 346370
id: layer_128_3_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 12452.5
id: layer_128_3_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 14057.9
id: layer_128_3_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 4366.73
id: layer_128_3_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_3_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 1279.83
id: layer_128_3_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.15884e+07
id: layer_128_3_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 69560.6
*************************  Network stats  ************************* layer_128_3_conv3
id: layer_128_3_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 662.531
id: layer_128_3_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 225.578
id: layer_128_3_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 57.4557
id: layer_128_3_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 14.2708
id: layer_128_3_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 14.2708
id: layer_128_3_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 57.4557
id: layer_128_3_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 225.578
id: layer_128_3_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 662.531
*******************
Layer id: layer_128_4_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_4_conv1
id: layer_128_4_conv1 ,Total cycles spent on fwd pass compute: 673
id: layer_128_4_conv1 ,Total cycles spent on weight grad compute: 1064
id: layer_128_4_conv1 ,Total cycles spent on input grad compute: 607
id: layer_128_4_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_4_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_4_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_4_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_128_4_conv1 ,Total cycles spent on weight grad comm: 29415990
id: layer_128_4_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_4_conv1
id: layer_128_4_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_4_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 415162
id: layer_128_4_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 8935.39
id: layer_128_4_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 7762.58
id: layer_128_4_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 886.703
id: layer_128_4_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_4_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 1048.16
id: layer_128_4_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.15497e+07
id: layer_128_4_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 45827
*************************  Network stats  ************************* layer_128_4_conv1
id: layer_128_4_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 662.531
id: layer_128_4_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 225.578
id: layer_128_4_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 57.4557
id: layer_128_4_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 14.2708
id: layer_128_4_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 14.2708
id: layer_128_4_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 57.4557
id: layer_128_4_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 225.578
id: layer_128_4_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 662.531
*******************
Layer id: layer_128_4_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_4_conv2
id: layer_128_4_conv2 ,Total cycles spent on fwd pass compute: 1684
id: layer_128_4_conv2 ,Total cycles spent on weight grad compute: 1730
id: layer_128_4_conv2 ,Total cycles spent on input grad compute: 1518
id: layer_128_4_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_4_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_4_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_4_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_128_4_conv2 ,Total cycles spent on weight grad comm: 29493999
id: layer_128_4_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_4_conv2
id: layer_128_4_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_4_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.01263e+06
id: layer_128_4_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 3288.58
id: layer_128_4_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 2059.44
id: layer_128_4_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 2047.19
id: layer_128_4_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_128_4_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 2672.94
id: layer_128_4_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.11155e+07
id: layer_128_4_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 91199.9
*************************  Network stats  ************************* layer_128_4_conv2
id: layer_128_4_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 1489.59
id: layer_128_4_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 505.922
id: layer_128_4_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 128.203
id: layer_128_4_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 30.8021
id: layer_128_4_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 30.8021
id: layer_128_4_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 128.203
id: layer_128_4_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 505.922
id: layer_128_4_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 1489.59
*******************
Layer id: layer_128_4_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_128_4_conv3
id: layer_128_4_conv3 ,Total cycles spent on fwd pass compute: 817
id: layer_128_4_conv3 ,Total cycles spent on weight grad compute: 1331
id: layer_128_4_conv3 ,Total cycles spent on input grad compute: 673
id: layer_128_4_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_128_4_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_128_4_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_128_4_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_128_4_conv3 ,Total cycles spent on weight grad comm: 29414397
id: layer_128_4_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_128_4_conv3
id: layer_128_4_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_128_4_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 436871
id: layer_128_4_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 8670.78
id: layer_128_4_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 11601.3
id: layer_128_4_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 4696.23
id: layer_128_4_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 1.0625
id: layer_128_4_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 1060.22
id: layer_128_4_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.15323e+07
id: layer_128_4_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 37564.7
*************************  Network stats  ************************* layer_128_4_conv3
id: layer_128_4_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 662.531
id: layer_128_4_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 225.578
id: layer_128_4_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 57.4557
id: layer_128_4_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 14.2708
id: layer_128_4_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 14.2708
id: layer_128_4_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 57.4557
id: layer_128_4_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 225.578
id: layer_128_4_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 662.531
*******************
Layer id: layer_256_1_conv4
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_1_conv4
id: layer_256_1_conv4 ,Total cycles spent on fwd pass compute: 1472
id: layer_256_1_conv4 ,Total cycles spent on weight grad compute: 1002
id: layer_256_1_conv4 ,Total cycles spent on input grad compute: 1472
id: layer_256_1_conv4 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_1_conv4 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_1_conv4 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_1_conv4 ,Total cycles spent on fwd pass comm: 0
id: layer_256_1_conv4 ,Total cycles spent on weight grad comm: 30504836
id: layer_256_1_conv4 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_1_conv4
id: layer_256_1_conv4 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_1_conv4 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 3.11774e+06
id: layer_256_1_conv4 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 14156.8
id: layer_256_1_conv4 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 6792.55
id: layer_256_1_conv4 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 6276.72
id: layer_256_1_conv4 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 6.78125
id: layer_256_1_conv4 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 7527.39
id: layer_256_1_conv4 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.95395e+07
id: layer_256_1_conv4 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 598658
*************************  Network stats  ************************* layer_256_1_conv4
id: layer_256_1_conv4 ,Average cycles spent on network for phase 1 of algorithm (per message): 5293.75
id: layer_256_1_conv4 ,Average cycles spent on network for phase 2 of algorithm (per message): 1795.46
id: layer_256_1_conv4 ,Average cycles spent on network for phase 3 of algorithm (per message): 451.966
id: layer_256_1_conv4 ,Average cycles spent on network for phase 4 of algorithm (per message): 105.708
id: layer_256_1_conv4 ,Average cycles spent on network for phase 5 of algorithm (per message): 105.708
id: layer_256_1_conv4 ,Average cycles spent on network for phase 6 of algorithm (per message): 451.966
id: layer_256_1_conv4 ,Average cycles spent on network for phase 7 of algorithm (per message): 1795.46
id: layer_256_1_conv4 ,Average cycles spent on network for phase 8 of algorithm (per message): 5293.75
*******************
Layer id: layer_256_1_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_1_conv1
id: layer_256_1_conv1 ,Total cycles spent on fwd pass compute: 920
id: layer_256_1_conv1 ,Total cycles spent on weight grad compute: 1064
id: layer_256_1_conv1 ,Total cycles spent on input grad compute: 817
id: layer_256_1_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_1_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_1_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_1_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_256_1_conv1 ,Total cycles spent on weight grad comm: 29472385
id: layer_256_1_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_1_conv1
id: layer_256_1_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_1_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 629633
id: layer_256_1_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 2385.89
id: layer_256_1_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 5320.11
id: layer_256_1_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 1324.02
id: layer_256_1_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_1_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 1655.66
id: layer_256_1_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.14035e+07
id: layer_256_1_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 91798
*************************  Network stats  ************************* layer_256_1_conv1
id: layer_256_1_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 1324.02
id: layer_256_1_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 449.701
id: layer_256_1_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 113.865
id: layer_256_1_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 27.25
id: layer_256_1_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 27.25
id: layer_256_1_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 113.865
id: layer_256_1_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 449.701
id: layer_256_1_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 1324.02
*******************
Layer id: layer_256_1_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_1_conv2
id: layer_256_1_conv2 ,Total cycles spent on fwd pass compute: 1656
id: layer_256_1_conv2 ,Total cycles spent on weight grad compute: 716
id: layer_256_1_conv2 ,Total cycles spent on input grad compute: 1656
id: layer_256_1_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_1_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_1_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_1_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_256_1_conv2 ,Total cycles spent on weight grad comm: 31374973
id: layer_256_1_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_1_conv2
id: layer_256_1_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_1_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 3.82691e+06
id: layer_256_1_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 12351.4
id: layer_256_1_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 8628.23
id: layer_256_1_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 7201.05
id: layer_256_1_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 6.78125
id: layer_256_1_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 8059.31
id: layer_256_1_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.90255e+07
id: layer_256_1_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 1.02304e+06
*************************  Network stats  ************************* layer_256_1_conv2
id: layer_256_1_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 5955.77
id: layer_256_1_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 2019.78
id: layer_256_1_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 508.391
id: layer_256_1_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 119.042
id: layer_256_1_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 119.042
id: layer_256_1_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 508.391
id: layer_256_1_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 2019.78
id: layer_256_1_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 5955.77
*******************
Layer id: layer_256_1_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_1_conv3
id: layer_256_1_conv3 ,Total cycles spent on fwd pass compute: 736
id: layer_256_1_conv3 ,Total cycles spent on weight grad compute: 736
id: layer_256_1_conv3 ,Total cycles spent on input grad compute: 920
id: layer_256_1_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_1_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_1_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_1_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_256_1_conv3 ,Total cycles spent on weight grad comm: 30254434
id: layer_256_1_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_1_conv3
id: layer_256_1_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_1_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.12649e+06
id: layer_256_1_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 11899.7
id: layer_256_1_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 10837
id: layer_256_1_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 2538.86
id: layer_256_1_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 11.8125
id: layer_256_1_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 3498.25
id: layer_256_1_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.10289e+07
id: layer_256_1_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 605828
*************************  Network stats  ************************* layer_256_1_conv3
id: layer_256_1_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_1_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_1_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_1_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_1_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_1_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_1_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_1_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_2_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_2_conv1
id: layer_256_2_conv1 ,Total cycles spent on fwd pass compute: 920
id: layer_256_2_conv1 ,Total cycles spent on weight grad compute: 383
id: layer_256_2_conv1 ,Total cycles spent on input grad compute: 736
id: layer_256_2_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_2_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_2_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_2_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_256_2_conv1 ,Total cycles spent on weight grad comm: 30177879
id: layer_256_2_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_2_conv1
id: layer_256_2_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_2_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.33226e+06
id: layer_256_2_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 4576.19
id: layer_256_2_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 3577.72
id: layer_256_2_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 2929.27
id: layer_256_2_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_2_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 3976.8
id: layer_256_2_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.08873e+07
id: layer_256_2_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 530611
*************************  Network stats  ************************* layer_256_2_conv1
id: layer_256_2_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_2_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_2_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_2_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_2_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_2_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_2_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_2_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_2_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_2_conv2
id: layer_256_2_conv2 ,Total cycles spent on fwd pass compute: 1656
id: layer_256_2_conv2 ,Total cycles spent on weight grad compute: 716
id: layer_256_2_conv2 ,Total cycles spent on input grad compute: 1656
id: layer_256_2_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_2_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_2_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_2_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_256_2_conv2 ,Total cycles spent on weight grad comm: 31235842
id: layer_256_2_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_2_conv2
id: layer_256_2_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_2_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 4.21859e+06
id: layer_256_2_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 15223
id: layer_256_2_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 7072.27
id: layer_256_2_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 8449.09
id: layer_256_2_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_2_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 9251.02
id: layer_256_2_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.87761e+07
id: layer_256_2_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 904126
*************************  Network stats  ************************* layer_256_2_conv2
id: layer_256_2_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 5955.77
id: layer_256_2_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 2019.78
id: layer_256_2_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 508.391
id: layer_256_2_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 119.042
id: layer_256_2_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 119.042
id: layer_256_2_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 508.391
id: layer_256_2_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 2019.78
id: layer_256_2_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 5955.77
*******************
Layer id: layer_256_2_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_2_conv3
id: layer_256_2_conv3 ,Total cycles spent on fwd pass compute: 736
id: layer_256_2_conv3 ,Total cycles spent on weight grad compute: 736
id: layer_256_2_conv3 ,Total cycles spent on input grad compute: 920
id: layer_256_2_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_2_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_2_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_2_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_256_2_conv3 ,Total cycles spent on weight grad comm: 30255005
id: layer_256_2_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_2_conv3
id: layer_256_2_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_2_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.46161e+06
id: layer_256_2_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 6273.31
id: layer_256_2_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 18988.8
id: layer_256_2_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 3971.52
id: layer_256_2_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_2_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 4016.34
id: layer_256_2_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.08015e+07
id: layer_256_2_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 455152
*************************  Network stats  ************************* layer_256_2_conv3
id: layer_256_2_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_2_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_2_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_2_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_2_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_2_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_2_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_2_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_3_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_3_conv1
id: layer_256_3_conv1 ,Total cycles spent on fwd pass compute: 920
id: layer_256_3_conv1 ,Total cycles spent on weight grad compute: 383
id: layer_256_3_conv1 ,Total cycles spent on input grad compute: 736
id: layer_256_3_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_3_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_3_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_3_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_256_3_conv1 ,Total cycles spent on weight grad comm: 30054736
id: layer_256_3_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_3_conv1
id: layer_256_3_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_3_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.67832e+06
id: layer_256_3_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 5477.27
id: layer_256_3_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 3933.59
id: layer_256_3_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 4873.45
id: layer_256_3_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_3_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 3726.56
id: layer_256_3_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.0649e+07
id: layer_256_3_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 371532
*************************  Network stats  ************************* layer_256_3_conv1
id: layer_256_3_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_3_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_3_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_3_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_3_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_3_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_3_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_3_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_3_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_3_conv2
id: layer_256_3_conv2 ,Total cycles spent on fwd pass compute: 1656
id: layer_256_3_conv2 ,Total cycles spent on weight grad compute: 716
id: layer_256_3_conv2 ,Total cycles spent on input grad compute: 1656
id: layer_256_3_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_3_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_3_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_3_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_256_3_conv2 ,Total cycles spent on weight grad comm: 31107017
id: layer_256_3_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_3_conv2
id: layer_256_3_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_3_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 4.60842e+06
id: layer_256_3_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 12996.8
id: layer_256_3_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 9256.31
id: layer_256_3_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 8511.89
id: layer_256_3_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_3_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 9576.75
id: layer_256_3_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.84849e+07
id: layer_256_3_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 774879
*************************  Network stats  ************************* layer_256_3_conv2
id: layer_256_3_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 5955.77
id: layer_256_3_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 2019.78
id: layer_256_3_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 508.391
id: layer_256_3_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 119.042
id: layer_256_3_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 119.042
id: layer_256_3_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 508.391
id: layer_256_3_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 2019.78
id: layer_256_3_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 5955.77
*******************
Layer id: layer_256_3_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_3_conv3
id: layer_256_3_conv3 ,Total cycles spent on fwd pass compute: 736
id: layer_256_3_conv3 ,Total cycles spent on weight grad compute: 736
id: layer_256_3_conv3 ,Total cycles spent on input grad compute: 920
id: layer_256_3_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_3_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_3_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_3_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_256_3_conv3 ,Total cycles spent on weight grad comm: 30255576
id: layer_256_3_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_3_conv3
id: layer_256_3_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_3_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.82547e+06
id: layer_256_3_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 7468.64
id: layer_256_3_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 11763.5
id: layer_256_3_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 4640.05
id: layer_256_3_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_3_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 4341.27
id: layer_256_3_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.05363e+07
id: layer_256_3_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 326424
*************************  Network stats  ************************* layer_256_3_conv3
id: layer_256_3_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_3_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_3_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_3_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_3_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_3_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_3_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_3_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_4_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_4_conv1
id: layer_256_4_conv1 ,Total cycles spent on fwd pass compute: 920
id: layer_256_4_conv1 ,Total cycles spent on weight grad compute: 383
id: layer_256_4_conv1 ,Total cycles spent on input grad compute: 736
id: layer_256_4_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_4_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_4_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_4_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_256_4_conv1 ,Total cycles spent on weight grad comm: 29934881
id: layer_256_4_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_4_conv1
id: layer_256_4_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_4_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.9951e+06
id: layer_256_4_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 6597.67
id: layer_256_4_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 11975.3
id: layer_256_4_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 3514.5
id: layer_256_4_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_4_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 3938.81
id: layer_256_4_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.0413e+07
id: layer_256_4_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 285218
*************************  Network stats  ************************* layer_256_4_conv1
id: layer_256_4_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_4_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_4_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_4_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_4_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_4_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_4_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_4_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_4_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_4_conv2
id: layer_256_4_conv2 ,Total cycles spent on fwd pass compute: 1656
id: layer_256_4_conv2 ,Total cycles spent on weight grad compute: 716
id: layer_256_4_conv2 ,Total cycles spent on input grad compute: 1656
id: layer_256_4_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_4_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_4_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_4_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_256_4_conv2 ,Total cycles spent on weight grad comm: 30957580
id: layer_256_4_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_4_conv2
id: layer_256_4_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_4_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 5.00012e+06
id: layer_256_4_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 17396.1
id: layer_256_4_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 9319.75
id: layer_256_4_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 10483.7
id: layer_256_4_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_4_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 12705.5
id: layer_256_4_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.81896e+07
id: layer_256_4_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 653518
*************************  Network stats  ************************* layer_256_4_conv2
id: layer_256_4_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 5955.77
id: layer_256_4_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 2019.78
id: layer_256_4_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 508.391
id: layer_256_4_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 119.042
id: layer_256_4_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 119.042
id: layer_256_4_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 508.391
id: layer_256_4_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 2019.78
id: layer_256_4_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 5955.77
*******************
Layer id: layer_256_4_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_4_conv3
id: layer_256_4_conv3 ,Total cycles spent on fwd pass compute: 736
id: layer_256_4_conv3 ,Total cycles spent on weight grad compute: 736
id: layer_256_4_conv3 ,Total cycles spent on input grad compute: 920
id: layer_256_4_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_4_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_4_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_4_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_256_4_conv3 ,Total cycles spent on weight grad comm: 30256147
id: layer_256_4_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_4_conv3
id: layer_256_4_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_4_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 2.16357e+06
id: layer_256_4_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 7857.64
id: layer_256_4_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 12247.6
id: layer_256_4_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 3122.84
id: layer_256_4_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_4_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 4174.67
id: layer_256_4_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.02917e+07
id: layer_256_4_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 255734
*************************  Network stats  ************************* layer_256_4_conv3
id: layer_256_4_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_4_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_4_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_4_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_4_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_4_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_4_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_4_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_5_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_5_conv1
id: layer_256_5_conv1 ,Total cycles spent on fwd pass compute: 920
id: layer_256_5_conv1 ,Total cycles spent on weight grad compute: 383
id: layer_256_5_conv1 ,Total cycles spent on input grad compute: 736
id: layer_256_5_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_5_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_5_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_5_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_256_5_conv1 ,Total cycles spent on weight grad comm: 29815026
id: layer_256_5_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_5_conv1
id: layer_256_5_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_5_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 2.368e+06
id: layer_256_5_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 9775.42
id: layer_256_5_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 5778.53
id: layer_256_5_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 5089.44
id: layer_256_5_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_5_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 5558.64
id: layer_256_5_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.01371e+07
id: layer_256_5_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 190289
*************************  Network stats  ************************* layer_256_5_conv1
id: layer_256_5_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_5_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_5_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_5_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_5_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_5_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_5_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_5_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_5_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_5_conv2
id: layer_256_5_conv2 ,Total cycles spent on fwd pass compute: 1656
id: layer_256_5_conv2 ,Total cycles spent on weight grad compute: 716
id: layer_256_5_conv2 ,Total cycles spent on input grad compute: 1656
id: layer_256_5_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_5_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_5_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_5_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_256_5_conv2 ,Total cycles spent on weight grad comm: 30818449
id: layer_256_5_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_5_conv2
id: layer_256_5_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_5_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 5.39464e+06
id: layer_256_5_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 18700.3
id: layer_256_5_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 9668.08
id: layer_256_5_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 10067.1
id: layer_256_5_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_5_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 11284.7
id: layer_256_5_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.7896e+07
id: layer_256_5_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 545613
*************************  Network stats  ************************* layer_256_5_conv2
id: layer_256_5_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 5955.77
id: layer_256_5_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 2019.78
id: layer_256_5_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 508.391
id: layer_256_5_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 119.042
id: layer_256_5_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 119.042
id: layer_256_5_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 508.391
id: layer_256_5_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 2019.78
id: layer_256_5_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 5955.77
*******************
Layer id: layer_256_5_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_5_conv3
id: layer_256_5_conv3 ,Total cycles spent on fwd pass compute: 736
id: layer_256_5_conv3 ,Total cycles spent on weight grad compute: 736
id: layer_256_5_conv3 ,Total cycles spent on input grad compute: 920
id: layer_256_5_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_5_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_5_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_5_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_256_5_conv3 ,Total cycles spent on weight grad comm: 30256718
id: layer_256_5_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_5_conv3
id: layer_256_5_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_5_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 2.50548e+06
id: layer_256_5_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 8268.66
id: layer_256_5_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 13115.6
id: layer_256_5_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 6430.19
id: layer_256_5_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_5_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 4805.17
id: layer_256_5_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 2.00325e+07
id: layer_256_5_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 172601
*************************  Network stats  ************************* layer_256_5_conv3
id: layer_256_5_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_5_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_5_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_5_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_5_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_5_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_5_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_5_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_6_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_6_conv1
id: layer_256_6_conv1 ,Total cycles spent on fwd pass compute: 920
id: layer_256_6_conv1 ,Total cycles spent on weight grad compute: 383
id: layer_256_6_conv1 ,Total cycles spent on input grad compute: 736
id: layer_256_6_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_6_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_6_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_6_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_256_6_conv1 ,Total cycles spent on weight grad comm: 29827473
id: layer_256_6_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_6_conv1
id: layer_256_6_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_6_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 2.71732e+06
id: layer_256_6_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 8223.09
id: layer_256_6_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 5504.38
id: layer_256_6_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 4811.48
id: layer_256_6_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_6_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 4679.86
id: layer_256_6_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.9879e+07
id: layer_256_6_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 114765
*************************  Network stats  ************************* layer_256_6_conv1
id: layer_256_6_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_6_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_6_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_6_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_6_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_6_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_6_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_6_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_256_6_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_6_conv2
id: layer_256_6_conv2 ,Total cycles spent on fwd pass compute: 1656
id: layer_256_6_conv2 ,Total cycles spent on weight grad compute: 716
id: layer_256_6_conv2 ,Total cycles spent on input grad compute: 1656
id: layer_256_6_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_6_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_6_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_6_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_256_6_conv2 ,Total cycles spent on weight grad comm: 30689624
id: layer_256_6_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_6_conv2
id: layer_256_6_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_6_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 5.78773e+06
id: layer_256_6_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 16588.6
id: layer_256_6_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 12204.6
id: layer_256_6_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 12795.3
id: layer_256_6_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_6_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 14901.7
id: layer_256_6_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.75955e+07
id: layer_256_6_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 438326
*************************  Network stats  ************************* layer_256_6_conv2
id: layer_256_6_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 5955.77
id: layer_256_6_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 2019.78
id: layer_256_6_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 508.391
id: layer_256_6_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 119.042
id: layer_256_6_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 119.042
id: layer_256_6_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 508.391
id: layer_256_6_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 2019.78
id: layer_256_6_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 5955.77
*******************
Layer id: layer_256_6_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_256_6_conv3
id: layer_256_6_conv3 ,Total cycles spent on fwd pass compute: 3271
id: layer_256_6_conv3 ,Total cycles spent on weight grad compute: 736
id: layer_256_6_conv3 ,Total cycles spent on input grad compute: 920
id: layer_256_6_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_256_6_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_256_6_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_256_6_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_256_6_conv3 ,Total cycles spent on weight grad comm: 29884709
id: layer_256_6_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_256_6_conv3
id: layer_256_6_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_256_6_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 2.88976e+06
id: layer_256_6_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 8980.14
id: layer_256_6_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 6607.56
id: layer_256_6_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 6668.97
id: layer_256_6_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_256_6_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 4739.86
id: layer_256_6_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.97495e+07
id: layer_256_6_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 67160.1
*************************  Network stats  ************************* layer_256_6_conv3
id: layer_256_6_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 2647.27
id: layer_256_6_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 898.286
id: layer_256_6_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 226.594
id: layer_256_6_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 53.4375
id: layer_256_6_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 53.4375
id: layer_256_6_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 226.594
id: layer_256_6_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 898.286
id: layer_256_6_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 2647.27
*******************
Layer id: layer_512_1_conv4
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_1_conv4
id: layer_512_1_conv4 ,Total cycles spent on fwd pass compute: 4667
id: layer_512_1_conv4 ,Total cycles spent on weight grad compute: 2764
id: layer_512_1_conv4 ,Total cycles spent on input grad compute: 4667
id: layer_512_1_conv4 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_1_conv4 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_1_conv4 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_1_conv4 ,Total cycles spent on fwd pass comm: 0
id: layer_512_1_conv4 ,Total cycles spent on weight grad comm: 33724830
id: layer_512_1_conv4 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_1_conv4
id: layer_512_1_conv4 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_1_conv4 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.13379e+07
id: layer_512_1_conv4 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 46515.7
id: layer_512_1_conv4 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 29684.4
id: layer_512_1_conv4 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 22472.7
id: layer_512_1_conv4 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_512_1_conv4 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 30889
id: layer_512_1_conv4 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.33435e+07
id: layer_512_1_conv4 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 1.6706e+06
*************************  Network stats  ************************* layer_512_1_conv4
id: layer_512_1_conv4 ,Average cycles spent on network for phase 1 of algorithm (per message): 21172.8
id: layer_512_1_conv4 ,Average cycles spent on network for phase 2 of algorithm (per message): 7178.49
id: layer_512_1_conv4 ,Average cycles spent on network for phase 3 of algorithm (per message): 1803.77
id: layer_512_1_conv4 ,Average cycles spent on network for phase 4 of algorithm (per message): 418.458
id: layer_512_1_conv4 ,Average cycles spent on network for phase 5 of algorithm (per message): 418.458
id: layer_512_1_conv4 ,Average cycles spent on network for phase 6 of algorithm (per message): 1803.77
id: layer_512_1_conv4 ,Average cycles spent on network for phase 7 of algorithm (per message): 7178.49
id: layer_512_1_conv4 ,Average cycles spent on network for phase 8 of algorithm (per message): 21172.8
*******************
Layer id: layer_512_1_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_1_conv1
id: layer_512_1_conv1 ,Total cycles spent on fwd pass compute: 3680
id: layer_512_1_conv1 ,Total cycles spent on weight grad compute: 767
id: layer_512_1_conv1 ,Total cycles spent on input grad compute: 3271
id: layer_512_1_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_1_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_1_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_1_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_512_1_conv1 ,Total cycles spent on weight grad comm: 30419893
id: layer_512_1_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_1_conv1
id: layer_512_1_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_1_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 3.45166e+06
id: layer_512_1_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 7767.5
id: layer_512_1_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 16296.2
id: layer_512_1_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 5930.14
id: layer_512_1_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_512_1_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 8045.39
id: layer_512_1_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.93226e+07
id: layer_512_1_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 486981
*************************  Network stats  ************************* layer_512_1_conv1
id: layer_512_1_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 5293.75
id: layer_512_1_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 1795.46
id: layer_512_1_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 451.966
id: layer_512_1_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 105.708
id: layer_512_1_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 105.708
id: layer_512_1_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 451.966
id: layer_512_1_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 1795.46
id: layer_512_1_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 5293.75
*******************
Layer id: layer_512_1_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_1_conv2
id: layer_512_1_conv2 ,Total cycles spent on fwd pass compute: 5250
id: layer_512_1_conv2 ,Total cycles spent on weight grad compute: 2554
id: layer_512_1_conv2 ,Total cycles spent on input grad compute: 5250
id: layer_512_1_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_1_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_1_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_1_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_512_1_conv2 ,Total cycles spent on weight grad comm: 35467332
id: layer_512_1_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_1_conv2
id: layer_512_1_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_1_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.28674e+07
id: layer_512_1_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 75263.6
id: layer_512_1_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 32531.5
id: layer_512_1_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 29918
id: layer_512_1_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 52.4062
id: layer_512_1_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 37259
id: layer_512_1_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.2263e+07
id: layer_512_1_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 3.59661e+06
*************************  Network stats  ************************* layer_512_1_conv2
id: layer_512_1_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 23819
id: layer_512_1_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 8076.01
id: layer_512_1_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 2029.14
id: layer_512_1_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 470.76
id: layer_512_1_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 470.76
id: layer_512_1_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 2029.14
id: layer_512_1_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 8076.01
id: layer_512_1_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 23819
*******************
Layer id: layer_512_1_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_1_conv3
id: layer_512_1_conv3 ,Total cycles spent on fwd pass compute: 2333
id: layer_512_1_conv3 ,Total cycles spent on weight grad compute: 1699
id: layer_512_1_conv3 ,Total cycles spent on input grad compute: 3680
id: layer_512_1_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_1_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_1_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_1_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_512_1_conv3 ,Total cycles spent on weight grad comm: 32718877
id: layer_512_1_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_1_conv3
id: layer_512_1_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_1_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 6.24366e+06
id: layer_512_1_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 17205.2
id: layer_512_1_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 32385.7
id: layer_512_1_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 16088.3
id: layer_512_1_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 14.9688
id: layer_512_1_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 15957.5
id: layer_512_1_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.72646e+07
id: layer_512_1_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 1.63142e+06
*************************  Network stats  ************************* layer_512_1_conv3
id: layer_512_1_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 10586.8
id: layer_512_1_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 3589.81
id: layer_512_1_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 902.458
id: layer_512_1_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 209.99
id: layer_512_1_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 209.99
id: layer_512_1_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 902.458
id: layer_512_1_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 3589.81
id: layer_512_1_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 10586.8
*******************
Layer id: layer_512_2_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_2_conv1
id: layer_512_2_conv1 ,Total cycles spent on fwd pass compute: 3680
id: layer_512_2_conv1 ,Total cycles spent on weight grad compute: 1223
id: layer_512_2_conv1 ,Total cycles spent on input grad compute: 2333
id: layer_512_2_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_2_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_2_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_2_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_512_2_conv1 ,Total cycles spent on weight grad comm: 32447988
id: layer_512_2_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_2_conv1
id: layer_512_2_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_2_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 7.02646e+06
id: layer_512_2_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 19471
id: layer_512_2_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 17815.7
id: layer_512_2_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 10529.9
id: layer_512_2_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_512_2_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 12358.2
id: layer_512_2_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.66976e+07
id: layer_512_2_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 1.35294e+06
*************************  Network stats  ************************* layer_512_2_conv1
id: layer_512_2_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 10586.8
id: layer_512_2_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 3589.81
id: layer_512_2_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 902.458
id: layer_512_2_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 209.99
id: layer_512_2_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 209.99
id: layer_512_2_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 902.458
id: layer_512_2_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 3589.81
id: layer_512_2_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 10586.8
*******************
Layer id: layer_512_2_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_2_conv2
id: layer_512_2_conv2 ,Total cycles spent on fwd pass compute: 5250
id: layer_512_2_conv2 ,Total cycles spent on weight grad compute: 2554
id: layer_512_2_conv2 ,Total cycles spent on input grad compute: 5250
id: layer_512_2_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_2_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_2_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_2_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_512_2_conv2 ,Total cycles spent on weight grad comm: 34907235
id: layer_512_2_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_2_conv2
id: layer_512_2_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_2_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.45024e+07
id: layer_512_2_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 41710.8
id: layer_512_2_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 37368.3
id: layer_512_2_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 34367.2
id: layer_512_2_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_512_2_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 36782.5
id: layer_512_2_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.11132e+07
id: layer_512_2_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 2.58133e+06
*************************  Network stats  ************************* layer_512_2_conv2
id: layer_512_2_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 23819
id: layer_512_2_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 8076.01
id: layer_512_2_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 2029.14
id: layer_512_2_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 470.76
id: layer_512_2_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 470.76
id: layer_512_2_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 2029.14
id: layer_512_2_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 8076.01
id: layer_512_2_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 23819
*******************
Layer id: layer_512_2_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_2_conv3
id: layer_512_2_conv3 ,Total cycles spent on fwd pass compute: 2333
id: layer_512_2_conv3 ,Total cycles spent on weight grad compute: 1699
id: layer_512_2_conv3 ,Total cycles spent on input grad compute: 3680
id: layer_512_2_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_2_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_2_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_2_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_512_2_conv3 ,Total cycles spent on weight grad comm: 32717306
id: layer_512_2_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_2_conv3
id: layer_512_2_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_2_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 7.59923e+06
id: layer_512_2_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 23738
id: layer_512_2_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 33758
id: layer_512_2_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 18388.2
id: layer_512_2_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 14.9688
id: layer_512_2_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 16256.5
id: layer_512_2_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.62642e+07
id: layer_512_2_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 1.13528e+06
*************************  Network stats  ************************* layer_512_2_conv3
id: layer_512_2_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 10586.8
id: layer_512_2_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 3589.81
id: layer_512_2_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 902.458
id: layer_512_2_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 209.99
id: layer_512_2_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 209.99
id: layer_512_2_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 902.458
id: layer_512_2_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 3589.81
id: layer_512_2_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 10586.8
*******************
Layer id: layer_512_3_conv1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_3_conv1
id: layer_512_3_conv1 ,Total cycles spent on fwd pass compute: 3680
id: layer_512_3_conv1 ,Total cycles spent on weight grad compute: 1223
id: layer_512_3_conv1 ,Total cycles spent on input grad compute: 2333
id: layer_512_3_conv1 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_3_conv1 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_3_conv1 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_3_conv1 ,Total cycles spent on fwd pass comm: 0
id: layer_512_3_conv1 ,Total cycles spent on weight grad comm: 31970201
id: layer_512_3_conv1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_3_conv1
id: layer_512_3_conv1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_3_conv1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 8.39657e+06
id: layer_512_3_conv1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 31524.4
id: layer_512_3_conv1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 16851.5
id: layer_512_3_conv1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 15897.5
id: layer_512_3_conv1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_512_3_conv1 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 20340.3
id: layer_512_3_conv1 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.56607e+07
id: layer_512_3_conv1 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 897982
*************************  Network stats  ************************* layer_512_3_conv1
id: layer_512_3_conv1 ,Average cycles spent on network for phase 1 of algorithm (per message): 10586.8
id: layer_512_3_conv1 ,Average cycles spent on network for phase 2 of algorithm (per message): 3589.81
id: layer_512_3_conv1 ,Average cycles spent on network for phase 3 of algorithm (per message): 902.458
id: layer_512_3_conv1 ,Average cycles spent on network for phase 4 of algorithm (per message): 209.99
id: layer_512_3_conv1 ,Average cycles spent on network for phase 5 of algorithm (per message): 209.99
id: layer_512_3_conv1 ,Average cycles spent on network for phase 6 of algorithm (per message): 902.458
id: layer_512_3_conv1 ,Average cycles spent on network for phase 7 of algorithm (per message): 3589.81
id: layer_512_3_conv1 ,Average cycles spent on network for phase 8 of algorithm (per message): 10586.8
*******************
Layer id: layer_512_3_conv2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_3_conv2
id: layer_512_3_conv2 ,Total cycles spent on fwd pass compute: 5250
id: layer_512_3_conv2 ,Total cycles spent on weight grad compute: 2554
id: layer_512_3_conv2 ,Total cycles spent on input grad compute: 5250
id: layer_512_3_conv2 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_3_conv2 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_3_conv2 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_3_conv2 ,Total cycles spent on fwd pass comm: 0
id: layer_512_3_conv2 ,Total cycles spent on weight grad comm: 34347138
id: layer_512_3_conv2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_3_conv2
id: layer_512_3_conv2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_3_conv2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 1.7171e+07
id: layer_512_3_conv2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 149510
id: layer_512_3_conv2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 52578.2
id: layer_512_3_conv2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 47126.3
id: layer_512_3_conv2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_512_3_conv2 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 36466.9
id: layer_512_3_conv2 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 9.63335e+06
id: layer_512_3_conv2 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 1.55329e+06
*************************  Network stats  ************************* layer_512_3_conv2
id: layer_512_3_conv2 ,Average cycles spent on network for phase 1 of algorithm (per message): 23819
id: layer_512_3_conv2 ,Average cycles spent on network for phase 2 of algorithm (per message): 8076.01
id: layer_512_3_conv2 ,Average cycles spent on network for phase 3 of algorithm (per message): 2029.14
id: layer_512_3_conv2 ,Average cycles spent on network for phase 4 of algorithm (per message): 470.76
id: layer_512_3_conv2 ,Average cycles spent on network for phase 5 of algorithm (per message): 470.76
id: layer_512_3_conv2 ,Average cycles spent on network for phase 6 of algorithm (per message): 2029.14
id: layer_512_3_conv2 ,Average cycles spent on network for phase 7 of algorithm (per message): 8076.01
id: layer_512_3_conv2 ,Average cycles spent on network for phase 8 of algorithm (per message): 23819
*******************
Layer id: layer_512_3_conv3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* layer_512_3_conv3
id: layer_512_3_conv3 ,Total cycles spent on fwd pass compute: 3194
id: layer_512_3_conv3 ,Total cycles spent on weight grad compute: 1699
id: layer_512_3_conv3 ,Total cycles spent on input grad compute: 3680
id: layer_512_3_conv3 ,Total cycles spent idle waiting for fwd finish: 0
id: layer_512_3_conv3 ,Total cycles spent idle waiting for weight grad finish: 0
id: layer_512_3_conv3 ,Total cycles spent idle waiting for input grad finish: 0
id: layer_512_3_conv3 ,Total cycles spent on fwd pass comm: 0
id: layer_512_3_conv3 ,Total cycles spent on weight grad comm: 31744987
id: layer_512_3_conv3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* layer_512_3_conv3
id: layer_512_3_conv3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: layer_512_3_conv3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 9.09879e+06
id: layer_512_3_conv3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 24055.6
id: layer_512_3_conv3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 17903.4
id: layer_512_3_conv3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 17276.9
id: layer_512_3_conv3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
id: layer_512_3_conv3 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 19207.4
id: layer_512_3_conv3 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.51408e+07
id: layer_512_3_conv3 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 728395
*************************  Network stats  ************************* layer_512_3_conv3
id: layer_512_3_conv3 ,Average cycles spent on network for phase 1 of algorithm (per message): 10586.8
id: layer_512_3_conv3 ,Average cycles spent on network for phase 2 of algorithm (per message): 3589.81
id: layer_512_3_conv3 ,Average cycles spent on network for phase 3 of algorithm (per message): 902.458
id: layer_512_3_conv3 ,Average cycles spent on network for phase 4 of algorithm (per message): 209.99
id: layer_512_3_conv3 ,Average cycles spent on network for phase 5 of algorithm (per message): 209.99
id: layer_512_3_conv3 ,Average cycles spent on network for phase 6 of algorithm (per message): 902.458
id: layer_512_3_conv3 ,Average cycles spent on network for phase 7 of algorithm (per message): 3589.81
id: layer_512_3_conv3 ,Average cycles spent on network for phase 8 of algorithm (per message): 10586.8
*******************
Layer id: fc1000
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* fc1000
id: fc1000 ,Total cycles spent on fwd pass compute: 4268
id: fc1000 ,Total cycles spent on weight grad compute: 2397
id: fc1000 ,Total cycles spent on input grad compute: 7204
id: fc1000 ,Total cycles spent idle waiting for fwd finish: 0
id: fc1000 ,Total cycles spent idle waiting for weight grad finish: 0
id: fc1000 ,Total cycles spent idle waiting for input grad finish: 0
id: fc1000 ,Total cycles spent on fwd pass comm: 0
id: fc1000 ,Total cycles spent on weight grad comm: 33262278
id: fc1000 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* fc1000
id: fc1000 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: fc1000 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 9.33523e+06
id: fc1000 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 125139
id: fc1000 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 30031.8
id: fc1000 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 25935.2
id: fc1000 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 18.4688
id: fc1000 ,Average cycles spent on queuing for phase 6 of algorithm (per chunk): 21817.2
id: fc1000 ,Average cycles spent on queuing for phase 7 of algorithm (per chunk): 1.48179e+07
id: fc1000 ,Average cycles spent on queuing for phase 8 of algorithm (per chunk): 1.59023e+06
*************************  Network stats  ************************* fc1000
id: fc1000 ,Average cycles spent on network for phase 1 of algorithm (per message): 20676.3
id: fc1000 ,Average cycles spent on network for phase 2 of algorithm (per message): 7010.57
id: fc1000 ,Average cycles spent on network for phase 3 of algorithm (per message): 1761.61
id: fc1000 ,Average cycles spent on network for phase 4 of algorithm (per message): 408.677
id: fc1000 ,Average cycles spent on network for phase 5 of algorithm (per message): 408.677
id: fc1000 ,Average cycles spent on network for phase 6 of algorithm (per message): 1761.61
id: fc1000 ,Average cycles spent on network for phase 7 of algorithm (per message): 7010.57
id: fc1000 ,Average cycles spent on network for phase 8 of algorithm (per message): 20676.3
*************************  Chunk Stats Per Logical Dimension (for all layers) ************************* fc1000
 ,Average chunk latency for logical dimension  1 of topology: 4535.76
 ,Average chunk latency for logical dimension  2 of topology: 4589.62
 ,Average chunk latency for logical dimension  3 of topology: 4712.46
 ,Average chunk latency for logical dimension  4 of topology: 4553.89
*************************
all passes finished at time: 35617193, id of first layer: conv1
path to create csvs is: /Users/aakashsharma/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/work/astra-sim/examples/../results/DDL_themis/Resnet50_DataParallel/
success in openning file
*****
Time to exit: Mon Dec  5 01:37:58 2022
all-reduce Collective implementation: ring_halvingDoubling_halvingDoubling_halvingDoubling
reduce-scatter Collective implementation: ring_halvingDoubling_halvingDoubling_halvingDoubling
all-gather Collective implementation: ring_halvingDoubling_halvingDoubling_halvingDoubling
all-to-all Collective implementation: direct_direct_direct_direct
Collective optimization: localBWAware
Total sim duration: 0:35 hours
Total streams injected: 3456
Total streams finished: 3456
Percentage of finished streams: 100 %
*****
Exiting

[Analytical, main] Total Cost: $1.40941e+06
