[CostModel] Added TileToTileLink: 32
(T-T Link) Added cost: 8000 (BW: 250, count: 32, unit_cost: 50)
[HierarchicalTopology, constructor] [Warning] Links-count at dimension 1 (FullyConnected) has 8 links (not a multiple of 7).
[CostModel] Added NVLink: 112
(NVLink) Added cost: 5600 (BW: 25, count: 112, unit_cost: 100)
[CostModel] Added NVLink: 32
(NVLink) Added cost: 800 (BW: 12.5, count: 32, unit_cost: 100)
[CostModel] Added NPU: 32
(Resource) Added cost: 0 (count: 32, unit_cost: 0)
Success in opening system file
Var is: scheduling-policy: ,val is: LIFO
Var is: endpoint-delay: ,val is: 10
Var is: active-chunks-per-dimension: ,val is: 1
Var is: preferred-dataset-splits: ,val is: 1
Var is: boost-mode: ,val is: 0
Var is: all-reduce-implementation: ,val is: doubleBinaryTree_doubleBinaryTree_doubleBinaryTree
Var is: all-gather-implementation: ,val is: ring_ring_ring
Var is: reduce-scatter-implementation: ,val is: ring_ring_ring
Var is: all-to-all-implementation: ,val is: oneDirect
Var is: collective-optimization: ,val is: localBWAware
Var is:  ,val is: localBWAware
The final active chunks per dimension after allocating to queues is: 100000000
Node 0: Double binary tree created with total nodes: 2 ,start: 0 ,stride: 1
Node 0: Double binary tree created with total nodes: 8 ,start: 0 ,stride: 2
Node 0: Double binary tree created with total nodes: 2 ,start: 0 ,stride: 16
ring of node 0, id: 0 dimension: local total nodes in ring: 2 index in ring: 0 offset: 1total nodes in ring: 2
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 2total nodes in ring: 8
ring of node 0, id: 0 dimension: local total nodes in ring: 2 index in ring: 0 offset: 16total nodes in ring: 2
ring of node 0, id: 0 dimension: local total nodes in ring: 2 index in ring: 0 offset: 1total nodes in ring: 2
ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 2total nodes in ring: 8
ring of node 0, id: 0 dimension: local total nodes in ring: 2 index in ring: 0 offset: 16total nodes in ring: 2
ring of node 0, id: 0 dimension: local total nodes in ring: 32 index in ring: 0 offset: 1total nodes in ring: 32
total nodes: 32
LogGP model, the local reduction delay is: 1
LogGP model, the local reduction delay is: 1
LogGP model, the local reduction delay is: 1
LogGP model, the local reduction delay is: 1
Shared bus modeling enabled? false
LogGP model, the L is:0 ,o is: 0 ,g is: 0 ,G is: 0.0038
communication delay (in the case of disabled shared bus): 10
Shared bus modeling enabled? false
LogGP model, the L is:0 ,o is: 0 ,g is: 0 ,G is: 0
communication delay (in the case of disabled shared bus): 10
Success in opening workload file
****************** info: DLRM workload last bottom layer is: 4
id: Embedding , depen: -1 , wg_comp_time: 300
id: MLP_Bottom_0 , depen: -1 , wg_comp_time: 896
id: MLP_Bottom_1 , depen: -1 , wg_comp_time: 1792
id: MLP_Bottom_2 , depen: -1 , wg_comp_time: 1792
id: MLP_Bottom_3 , depen: -1 , wg_comp_time: 800
id: MLP_Top_0 , depen: -1 , wg_comp_time: 4480
id: MLP_Top_1 , depen: -1 , wg_comp_time: 1792
id: MLP_Top_2 , depen: -1 , wg_comp_time: 1792
type: HYBRID_DLRM ,num passes: 1 ,lines: 8 compute scale: 0.26 ,comm scale: 25
stat path: /Users/aakashsharma/work/astra-sim/examples/../results/DDL_3d// ,total rows: 6 ,stat row: 0
CSV path and filename: /Users/aakashsharma/work/astra-sim/examples/../results/DDL_3d//detailed.csv
Success in opening CSV file for writing the report.
CSV path and filename: /Users/aakashsharma/work/astra-sim/examples/../results/DDL_3d//EndToEnd.csv
Success in opening CSV file for writing the report.
CSV path and filename: /Users/aakashsharma/work/astra-sim/examples/../results/DDL_3d//backend_end_to_end.csv
Success in opening CSV file for writing the report.
CSV path and filename: /Users/aakashsharma/work/astra-sim/examples/../results/DDL_3d//backend_dim_info.csv
Success in opening CSV file for writing the report.
info: all-to-all forward pass collective issued for layer: Embedding, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
*************************layer changed to: 1
*************************layer changed to: 2
*************************layer changed to: 3
*************************layer changed to: 4
***** info: fwd pass comm collective for layer: Embedding is finished************
*************************layer changed to: 5
*************************layer changed to: 6
*************************layer changed to: 7
*************************layer changed to: 7
info: all-reduce weight grad collective issued for layer: MLP_Top_2 with size: 13107200, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
*************************layer changed to: 6 in ig
info: all-reduce weight grad collective issued for layer: MLP_Top_1 with size: 13107200, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
*************************layer changed to: 5 in ig
info: all-reduce weight grad collective issued for layer: MLP_Top_0 with size: 26624000, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
info: all-to-all input grad collective issued for layer: Embedding, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
*************************layer changed to: 4 in ig
info: all-reduce weight grad collective issued for layer: MLP_Bottom_3 with size: 409600, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
*************************layer changed to: 3 in ig
info: all-reduce weight grad collective issued for layer: MLP_Bottom_2 with size: 13107200, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
*************************layer changed to: 2 in ig
info: all-reduce weight grad collective issued for layer: MLP_Bottom_1 with size: 13107200, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
*************************layer changed to: 1 in ig
info: all-reduce weight grad collective issued for layer: MLP_Bottom_0 with size: 332800, involved dimensions:  1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
*************************layer changed to: 0 in ig
info: no weight grad collective for layer: Embedding
***** info: input gradient collective for layer: Embedding is finished************
pass: 0 finished at time: 58533
***** info: weight gradient collective for layer: MLP_Bottom_0 is finished************
***** info: weight gradient collective for layer: MLP_Bottom_1 is finished************
***** info: weight gradient collective for layer: MLP_Bottom_2 is finished************
***** info: weight gradient collective for layer: MLP_Bottom_3 is finished************
***** info: weight gradient collective for layer: MLP_Top_0 is finished************
***** info: weight gradient collective for layer: MLP_Top_1 is finished************
***** info: weight gradient collective for layer: MLP_Top_2 is finished************
*******************
Layer id: Embedding
Total collectives issued for this layer: 2
*************************  Workload stats  ************************* Embedding
id: Embedding ,Total cycles spent on fwd pass compute: 77
id: Embedding ,Total cycles spent on weight grad compute: 77
id: Embedding ,Total cycles spent on input grad compute: 0
id: Embedding ,Total cycles spent idle waiting for fwd finish: 1156
id: Embedding ,Total cycles spent idle waiting for weight grad finish: 0
id: Embedding ,Total cycles spent idle waiting for input grad finish: 44015
id: Embedding ,Total cycles spent on fwd pass comm: 3029
id: Embedding ,Total cycles spent on weight grad comm: 0
id: Embedding ,Total cycles spent on input grad comm: 47338
*************************  Queuing stats  ************************* Embedding
id: Embedding ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: Embedding ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 22154.5
*************************  Network stats  ************************* Embedding
id: Embedding ,Average cycles spent on network for phase 1 of algorithm (per message): 2799.13
*******************
Layer id: MLP_Bottom_0
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* MLP_Bottom_0
id: MLP_Bottom_0 ,Total cycles spent on fwd pass compute: 332
id: MLP_Bottom_0 ,Total cycles spent on weight grad compute: 232
id: MLP_Bottom_0 ,Total cycles spent on input grad compute: 206
id: MLP_Bottom_0 ,Total cycles spent idle waiting for fwd finish: 0
id: MLP_Bottom_0 ,Total cycles spent idle waiting for weight grad finish: 0
id: MLP_Bottom_0 ,Total cycles spent idle waiting for input grad finish: 0
id: MLP_Bottom_0 ,Total cycles spent on fwd pass comm: 0
id: MLP_Bottom_0 ,Total cycles spent on weight grad comm: 6051748
id: MLP_Bottom_0 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* MLP_Bottom_0
id: MLP_Bottom_0 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: MLP_Bottom_0 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 44202
id: MLP_Bottom_0 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 2.92595e+06
id: MLP_Bottom_0 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 902318
id: MLP_Bottom_0 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 2.0029e+06
id: MLP_Bottom_0 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
*************************  Network stats  ************************* MLP_Bottom_0
id: MLP_Bottom_0 ,Average cycles spent on network for phase 1 of algorithm (per message): 1248
id: MLP_Bottom_0 ,Average cycles spent on network for phase 2 of algorithm (per message): 74432
id: MLP_Bottom_0 ,Average cycles spent on network for phase 3 of algorithm (per message): 24904
id: MLP_Bottom_0 ,Average cycles spent on network for phase 4 of algorithm (per message): 37221
id: MLP_Bottom_0 ,Average cycles spent on network for phase 5 of algorithm (per message): 629
*******************
Layer id: MLP_Bottom_1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* MLP_Bottom_1
id: MLP_Bottom_1 ,Total cycles spent on fwd pass compute: 665
id: MLP_Bottom_1 ,Total cycles spent on weight grad compute: 465
id: MLP_Bottom_1 ,Total cycles spent on input grad compute: 665
id: MLP_Bottom_1 ,Total cycles spent idle waiting for fwd finish: 0
id: MLP_Bottom_1 ,Total cycles spent idle waiting for weight grad finish: 0
id: MLP_Bottom_1 ,Total cycles spent idle waiting for input grad finish: 0
id: MLP_Bottom_1 ,Total cycles spent on fwd pass comm: 0
id: MLP_Bottom_1 ,Total cycles spent on weight grad comm: 11961118
id: MLP_Bottom_1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* MLP_Bottom_1
id: MLP_Bottom_1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: MLP_Bottom_1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 46369
id: MLP_Bottom_1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 2.95146e+06
id: MLP_Bottom_1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 0
id: MLP_Bottom_1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 2.02783e+06
id: MLP_Bottom_1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
*************************  Network stats  ************************* MLP_Bottom_1
id: MLP_Bottom_1 ,Average cycles spent on network for phase 1 of algorithm (per message): 48916
id: MLP_Bottom_1 ,Average cycles spent on network for phase 2 of algorithm (per message): 2.93013e+06
id: MLP_Bottom_1 ,Average cycles spent on network for phase 3 of algorithm (per message): 488341
id: MLP_Bottom_1 ,Average cycles spent on network for phase 4 of algorithm (per message): 1.46503e+06
id: MLP_Bottom_1 ,Average cycles spent on network for phase 5 of algorithm (per message): 24424
*******************
Layer id: MLP_Bottom_2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* MLP_Bottom_2
id: MLP_Bottom_2 ,Total cycles spent on fwd pass compute: 665
id: MLP_Bottom_2 ,Total cycles spent on weight grad compute: 465
id: MLP_Bottom_2 ,Total cycles spent on input grad compute: 665
id: MLP_Bottom_2 ,Total cycles spent idle waiting for fwd finish: 0
id: MLP_Bottom_2 ,Total cycles spent idle waiting for weight grad finish: 0
id: MLP_Bottom_2 ,Total cycles spent idle waiting for input grad finish: 0
id: MLP_Bottom_2 ,Total cycles spent on fwd pass comm: 0
id: MLP_Bottom_2 ,Total cycles spent on weight grad comm: 14892396
id: MLP_Bottom_2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* MLP_Bottom_2
id: MLP_Bottom_2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: MLP_Bottom_2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 96437
id: MLP_Bottom_2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 5.90712e+06
id: MLP_Bottom_2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 0
id: MLP_Bottom_2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 1.95338e+06
id: MLP_Bottom_2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
*************************  Network stats  ************************* MLP_Bottom_2
id: MLP_Bottom_2 ,Average cycles spent on network for phase 1 of algorithm (per message): 48916
id: MLP_Bottom_2 ,Average cycles spent on network for phase 2 of algorithm (per message): 2.93013e+06
id: MLP_Bottom_2 ,Average cycles spent on network for phase 3 of algorithm (per message): 976750
id: MLP_Bottom_2 ,Average cycles spent on network for phase 4 of algorithm (per message): 1.46503e+06
id: MLP_Bottom_2 ,Average cycles spent on network for phase 5 of algorithm (per message): 24424
*******************
Layer id: MLP_Bottom_3
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* MLP_Bottom_3
id: MLP_Bottom_3 ,Total cycles spent on fwd pass compute: 207
id: MLP_Bottom_3 ,Total cycles spent on weight grad compute: 207
id: MLP_Bottom_3 ,Total cycles spent on input grad compute: 332
id: MLP_Bottom_3 ,Total cycles spent idle waiting for fwd finish: 0
id: MLP_Bottom_3 ,Total cycles spent idle waiting for weight grad finish: 0
id: MLP_Bottom_3 ,Total cycles spent idle waiting for input grad finish: 0
id: MLP_Bottom_3 ,Total cycles spent on fwd pass comm: 0
id: MLP_Bottom_3 ,Total cycles spent on weight grad comm: 20980332
id: MLP_Bottom_3 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* MLP_Bottom_3
id: MLP_Bottom_3 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: MLP_Bottom_3 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 146172
id: MLP_Bottom_3 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 1.4696e+07
id: MLP_Bottom_3 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 0
id: MLP_Bottom_3 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 5.92114e+06
id: MLP_Bottom_3 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
*************************  Network stats  ************************* MLP_Bottom_3
id: MLP_Bottom_3 ,Average cycles spent on network for phase 1 of algorithm (per message): 1534
id: MLP_Bottom_3 ,Average cycles spent on network for phase 2 of algorithm (per message): 91598
id: MLP_Bottom_3 ,Average cycles spent on network for phase 3 of algorithm (per message): 15318
id: MLP_Bottom_3 ,Average cycles spent on network for phase 4 of algorithm (per message): 45804
id: MLP_Bottom_3 ,Average cycles spent on network for phase 5 of algorithm (per message): 772
*******************
Layer id: MLP_Top_0
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* MLP_Top_0
id: MLP_Top_0 ,Total cycles spent on fwd pass compute: 1663
id: MLP_Top_0 ,Total cycles spent on weight grad compute: 1164
id: MLP_Top_0 ,Total cycles spent on input grad compute: 1663
id: MLP_Top_0 ,Total cycles spent idle waiting for fwd finish: 0
id: MLP_Top_0 ,Total cycles spent idle waiting for weight grad finish: 0
id: MLP_Top_0 ,Total cycles spent idle waiting for input grad finish: 0
id: MLP_Top_0 ,Total cycles spent on fwd pass comm: 0
id: MLP_Top_0 ,Total cycles spent on weight grad comm: 29963007
id: MLP_Top_0 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* MLP_Top_0
id: MLP_Top_0 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: MLP_Top_0 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 149598
id: MLP_Top_0 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 1.46883e+07
id: MLP_Top_0 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 0
id: MLP_Top_0 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 1.03783e+06
id: MLP_Top_0 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
*************************  Network stats  ************************* MLP_Top_0
id: MLP_Top_0 ,Average cycles spent on network for phase 1 of algorithm (per message): 99351
id: MLP_Top_0 ,Average cycles spent on network for phase 2 of algorithm (per message): 5.95177e+06
id: MLP_Top_0 ,Average cycles spent on network for phase 3 of algorithm (per message): 1.98391e+06
id: MLP_Top_0 ,Average cycles spent on network for phase 4 of algorithm (per message): 2.97581e+06
id: MLP_Top_0 ,Average cycles spent on network for phase 5 of algorithm (per message): 49601
*******************
Layer id: MLP_Top_1
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* MLP_Top_1
id: MLP_Top_1 ,Total cycles spent on fwd pass compute: 665
id: MLP_Top_1 ,Total cycles spent on weight grad compute: 465
id: MLP_Top_1 ,Total cycles spent on input grad compute: 665
id: MLP_Top_1 ,Total cycles spent idle waiting for fwd finish: 0
id: MLP_Top_1 ,Total cycles spent idle waiting for weight grad finish: 0
id: MLP_Top_1 ,Total cycles spent idle waiting for input grad finish: 0
id: MLP_Top_1 ,Total cycles spent on fwd pass comm: 0
id: MLP_Top_1 ,Total cycles spent on weight grad comm: 32844008
id: MLP_Top_1 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* MLP_Top_1
id: MLP_Top_1 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: MLP_Top_1 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 250800
id: MLP_Top_1 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 2.06827e+07
id: MLP_Top_1 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 0
id: MLP_Top_1 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 4.97502e+06
id: MLP_Top_1 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
*************************  Network stats  ************************* MLP_Top_1
id: MLP_Top_1 ,Average cycles spent on network for phase 1 of algorithm (per message): 48916
id: MLP_Top_1 ,Average cycles spent on network for phase 2 of algorithm (per message): 2.93013e+06
id: MLP_Top_1 ,Average cycles spent on network for phase 3 of algorithm (per message): 488341
id: MLP_Top_1 ,Average cycles spent on network for phase 4 of algorithm (per message): 1.46503e+06
id: MLP_Top_1 ,Average cycles spent on network for phase 5 of algorithm (per message): 24424
*******************
Layer id: MLP_Top_2
Total collectives issued for this layer: 1
*************************  Workload stats  ************************* MLP_Top_2
id: MLP_Top_2 ,Total cycles spent on fwd pass compute: 665
id: MLP_Top_2 ,Total cycles spent on weight grad compute: 465
id: MLP_Top_2 ,Total cycles spent on input grad compute: 665
id: MLP_Top_2 ,Total cycles spent idle waiting for fwd finish: 0
id: MLP_Top_2 ,Total cycles spent idle waiting for weight grad finish: 0
id: MLP_Top_2 ,Total cycles spent idle waiting for input grad finish: 0
id: MLP_Top_2 ,Total cycles spent on fwd pass comm: 0
id: MLP_Top_2 ,Total cycles spent on weight grad comm: 35775286
id: MLP_Top_2 ,Total cycles spent on input grad comm: 0
*************************  Queuing stats  ************************* MLP_Top_2
id: MLP_Top_2 ,Average cycles spent on queuing for phase 0 of algorithm (per chunk): 0
id: MLP_Top_2 ,Average cycles spent on queuing for phase 1 of algorithm (per chunk): 0
id: MLP_Top_2 ,Average cycles spent on queuing for phase 2 of algorithm (per chunk): 0
id: MLP_Top_2 ,Average cycles spent on queuing for phase 3 of algorithm (per chunk): 0
id: MLP_Top_2 ,Average cycles spent on queuing for phase 4 of algorithm (per chunk): 2.88398e+07
id: MLP_Top_2 ,Average cycles spent on queuing for phase 5 of algorithm (per chunk): 0
*************************  Network stats  ************************* MLP_Top_2
id: MLP_Top_2 ,Average cycles spent on network for phase 1 of algorithm (per message): 48916
id: MLP_Top_2 ,Average cycles spent on network for phase 2 of algorithm (per message): 2.93013e+06
id: MLP_Top_2 ,Average cycles spent on network for phase 3 of algorithm (per message): 976750
id: MLP_Top_2 ,Average cycles spent on network for phase 4 of algorithm (per message): 1.46503e+06
id: MLP_Top_2 ,Average cycles spent on network for phase 5 of algorithm (per message): 24424
*************************  Chunk Stats Per Logical Dimension (for all layers) ************************* MLP_Top_2
 ,Average chunk latency for logical dimension  1 of topology: 37608.5
 ,Average chunk latency for logical dimension  2 of topology: 2.54835e+06
 ,Average chunk latency for logical dimension  3 of topology: 849512
*************************
all passes finished at time: 35781854, id of first layer: Embedding
